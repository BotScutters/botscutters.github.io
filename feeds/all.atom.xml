<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bits 'n' Bots</title><link href="http://scottbutters.com/" rel="alternate"></link><link href="http://scottbutters.com/feeds/all.atom.xml" rel="self"></link><id>http://scottbutters.com/</id><updated>2019-06-18T11:00:00-07:00</updated><entry><title>AI of the Beholder</title><link href="http://scottbutters.com/articles/2019/06/07/AI-of-the-Beholder/" rel="alternate"></link><published>2019-06-07T14:00:00-07:00</published><updated>2019-06-18T11:00:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:scottbutters.com,2019-06-07:/articles/2019/06/07/AI-of-the-Beholder/</id><summary type="html">&lt;p&gt;We hear a lot nowadays about the many superpowers of AI, but we don't hear much about? Its highly refined aesthetic sensibilities. Well, that's what I'm here to change! I trained a convolutional neural net to look through thousands of photos taken from on top of the space needle and learn which ones are junk (as a baseline), and even to highlight the most beautiful of the bunch! But while the concept may be a bit of pet-project-floof, it's not without use case: think first-pass filter on a photographer's raw photos; think auto-curated album from your snap-happy vacation. Read on for more!&lt;/p&gt;</summary><content type="html">&lt;p&gt;We hear a lot nowadays about the many superpowers of AI, but we don't hear much about? Its highly refined aesthetic sensibilities. Well, that's what I'm here to change! I trained a convolutional neural net to look through thousands of photos taken from on top of the space needle and learn which ones are junk (as a baseline), and even to highlight the most beautiful of the bunch! But while the concept may be a bit of pet-project-floof, it's not without use case: think first-pass filter on a photographer's raw photos; think auto-curated album from your snap-happy vacation. &lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/beholder5.jpg" alt="ai-of-the-beholder" style="width:100%"&gt;
  &lt;figcaption&gt;You're so pretty, Seattle.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;For this project, I've trained a convolutional neural net to look through a gallery of images and then assign scores to each image according to it's quality, allowing a user to apply the algorithm as a first-pass filter to reduce the size of a gallery to review. As an added benefit, the scores can also be used to highlight the best amongst the images. The dataset used for this project is a collection of panoramic images taken of the Seattle skyline by a camera on top the Space Needle. I implemented the model in Keras using Xception architecture with transfer learning, instantiated with weights pre-trained on ImageNet. The convolutional base is reduced by a 2D global average pooling layer followed by a dropout layer and a fully connected layer reducing down to a single neuron with linear activation for the final prediction of an image's score. All but the top 4 blocks of the convolutional base were frozen during training. The top performing model used the following parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dropout rate: 0.6&lt;/li&gt;
&lt;li&gt;Optimizer: stochastic gradient descent, learning rate = 0.0001, momentum = 0.9, clip value = 0.5&lt;/li&gt;
&lt;li&gt;Loss function: mean squared error&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Tools&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;General&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Numpy &lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;Jupyter Notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cloud&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWS S3&lt;/li&gt;
&lt;li&gt;AWS EC2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keras&lt;/li&gt;
&lt;li&gt;TensorFlow&lt;/li&gt;
&lt;li&gt;TensorBoard&lt;/li&gt;
&lt;li&gt;Sci-kit Learn&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;li&gt;Seaborn&lt;/li&gt;
&lt;li&gt;Sci-kit Learn&lt;/li&gt;
&lt;li&gt;TensorBoard&lt;/li&gt;
&lt;li&gt;PIL&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Data Source&lt;/h1&gt;
&lt;p&gt;My data source for this project was the panoramic photos taken by the camera on top of the space needle. Check out their gallery &lt;a href="https://www.spaceneedle.com/webcam/"&gt;here&lt;/a&gt;. The camera has been taking one image for every ~10 minutes of daylight since the beginning of 2015, amounting to approximately 130,000 images now. After some sleuthing through the page's source code, I found links to the site's &lt;a href="https://spaceneedledev.com/panocam/assets/"&gt;filesystem&lt;/a&gt;, which they've left exposed. For each panorama photo, there's a folder has 17 high resolution slices that make up the panorama, as well as a low resolution (237x1820) thumbnail image with all the slices stitched together. Gold mine! I spun up an EC2 instance and wrote a script to crawl their entire file tree and duplicate all of the thumbnails (about 14 GB) into an AWS S3 bucket. Good to go!&lt;/p&gt;
&lt;h1&gt;Data Preparation&lt;/h1&gt;
&lt;p&gt;We can't very well have our model running around looking at photos without a goal, so next I needed to get some labels on my data. I started out treating this as a classification problem, with the goal of specifically labeling a photo as either beautiful or not. This is of course playing with devil, because what kind of fuzzy boundary around a category is that? How could that possibly be objective?What even &lt;em&gt;is&lt;/em&gt; beauty? Oh, these questions tumbled around in my brain and I reveled in the challenge of how best to approach it. As for actually getting these scores, I considered my options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mechanical Turk&lt;/strong&gt; - I &lt;em&gt;could&lt;/em&gt; devise an easy-to-follow scoring system and sick the crowd on it. So long as my instructions were clear and I had good accountability measures in place to make sure my turks were well-behaved and not turkeys or headless Selenium drivers, this would likely yield robust results. On the flip side, this will probably take me several hours to get set up, and costs money (and I'm not being paid for this).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Heroku App&lt;/strong&gt; - I could throw together a quick hot-or-not style app and send it to all my friends. Really this is the same solution as mechanical turk, but a bit closer to free. And perhaps it would have been the better option? But it's not what happened. Instead I opted for the ol'&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do it myself&lt;/strong&gt; - I didn't want to spend the money, and I had doubts about whether or not I'd be able to get the necessary scale of responses I wanted by leaning on my social network—how much time would any of them really voluntarily devote to scoring photo after photo of the Seattle skyline? And I didn't really want to make that ask. So I bit the bullet and did it myself.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wrote myself a scoring program in Jupyter notebook that I could use to quickly flip through and score photos. I thought about all the potential strategies. Implement a manual version of insertion sort? Treat it like a bracket and do lots of A vs B comparisons? These seemed like cool strategies, but also like the time complexity would kill me. I wanted a way to generate 2000+ scores in less than 10 hours of work with informative gradations and as high a degree of consistency as practical. That's…kind of a big ask. So here's how I did it.&lt;/p&gt;
&lt;h2&gt;Data Labeling&lt;/h2&gt;
&lt;p&gt;I randomly drew 4 images at a time from my S3 bucket, and then was faced with a prompt to enter the indices for the images I want to advance to the next round. It's like a tournament bracket…but because I'm a smart human that doesn't need strict rules, there's no constraint on how many choices I make in a round. This allowed me to avoid that obstacle of "what if they're both really similar!" Rather than always having to advance and discard some number, I instead kept track of my overall acceptance rate and tried to roughly peg that at a particular threshold. Starting out, I aimed for a 50/50 split, since as a baseline I wanted at least verify the model could work on a balanced dataset. The difference between these might be something like this:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/0beauty.jpg" alt="uggo" style="width:100%"&gt;
  &lt;figcaption&gt;Beauty: 0.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/1beauty.jpg" alt="yeah-you-pretty" style="width:100%"&gt;
  &lt;figcaption&gt;Beauty: 1.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Once I got a model predicting at &amp;gt; 90% accuracy on my split (more on that later), I decided to up the ante. I fed my positive class back into my labeling program for a second round. This time I went more aggressive and tried to select all the way up to the 95th percentile of photos. This tended to be a tight enough threshold that most of the images were low in defects and also had something &lt;em&gt;interesting&lt;/em&gt; about them, be it good lighting, dynamic clouds, a colorful sunset, absolute clarity...&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/2beauty.jpg" alt="lookin-good" style="width:100%"&gt;
  &lt;figcaption&gt;Beauty: 2.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;But that wasn't enough for me. I narrowed down one tier further to identify my top 2% of photos. These were the real upper crust. Still not always devoid of flaws, but generally in quite good shape and some fo the most compelling scenes.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/3beauty.jpg" alt="hot-stuff" style="width:100%"&gt;
  &lt;figcaption&gt;Beauty: 3.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;At this point, I knew I'd be running into a severe class imbalance issue without substantially increasing the number of photos I sifted through, so I left it here for the time being.&lt;/p&gt;
&lt;h2&gt;Data Labeling Round 2!&lt;/h2&gt;
&lt;p&gt;I was right, that class imbalance was an issue! Even after labelling over 3200 photos, a 64/16/20 train/crossval/test split meant that trying to identify the top 2% of images would give me a meager 11 positive class photos in my validation set, even with tools like image augmentation and oversampling at my disposal…I'm sorry, no, that's just not going to be enough. &lt;/p&gt;
&lt;h1&gt;Modeling&lt;/h1&gt;
&lt;p&gt;For the modeling, I decided to implement a convolutional neural network using Keras and Tensorflow. After surveying architectures implemented in Keras that also have pre-trained weights, I opted to use the Xception architecture based on it's combination of high reported accuracy and relatively low size and parameter counts as compared to the new old guard architectures like VGG 16/19. &lt;/p&gt;
&lt;p&gt;I chopped off the model at the latent vector output by the convolutional base and tacked my own predictor onto the end. I experimented with several potential arrangements, but ended up settling on prediction setup like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D global average pooling layer&lt;/li&gt;
&lt;li&gt;Dropout layer (rate = 0.6)&lt;/li&gt;
&lt;li&gt;Fully connected layer&lt;/li&gt;
&lt;li&gt;Output neuron(s)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As mentioned before, I initially started out treating this as a classification problem, and followed the advice of the Xception authors by setting up the output as two neurons with softmax activation. This allowed me to easily easily interpret my results according to whether or not a photo I'd classified as being categorically "beautiful" was also recognized as such by the model. To measure the performance, I implemented an F2 metric that would only be computed once per epoch (F2 because I wanted to place higher emphasis on recall than precision, as I'd prefer to do a little bit of extra sorting if it means I don't lose miss out on a real gem). I experimented with several optimizers and loss functions, but ultimately my best parameters were as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stochastic gradient descent: learning rate = 0.0001, momentum = 0.9, clip_value = 0.5&lt;/li&gt;
&lt;li&gt;Loss function: categorical cross entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ultimately, the best performance I got on this model while targeting the top 5% of photos was an F2 score of 0.27. Which was decent, but the more I fished around in the probability estimates and tried to determine what was hindering performance, the more I really started getting the itch that my problem had somehow transformed from a classification problem to a regression problem, albeit a very low resolution regression problem. Based on the mode by which I'd scored the data, I now had a 4-class dataset, but these classes were &lt;em&gt;ordinal&lt;/em&gt;. And not just that, but also their distances were actually somewhat meaningful. And my classification model was totally disregarding some of that information, and I didn't like that. I could treat it as a multi-label classification problem, but that doesn't account for the ordering.&lt;/p&gt;
&lt;p&gt;Pivot!&lt;/p&gt;
&lt;p&gt;So now I was building a regression model. But as it turns out, there's a simple way to do this that requires barely any change at all (to the model). To convert my prediction block to a regression, I changed the output from two neurons with softmax activation to one output with linear activation. And that's it! Well, mostly. Other things I changed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimizer: rmsprop&lt;/li&gt;
&lt;li&gt;Loss function: mean_squared_error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The only other changes that were required were modifications to the pipeline feeding the data into the model, such as transforming my labels into more useful target scores and adjusting the data generators to reference the correct variable.&lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;Overall, both models did a pretty good job of filtering out the bad photos. But how do we actually compare the performance of a regression model to a classification model? For this context, I decided to look at it through lens of time-savings. Supposing my goal is to have my model filter the dataset down to the smallest dataset it can while still maintaining the top 5% of photos, what percent gallery reduction can I achieve? &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Classification: 42% reduction in gallery size&lt;/li&gt;
&lt;li&gt;Regression: 24% reduction in gallery size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alas, despite all my hopes and dreams for the regression model, the classifier actually won out!&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In it's current state, the project has made significant progress towards demonstrating the potential for an intelligent neural network to be able to learn about our aesthetic sensibilities and filter the best photos from a chaotic collection. That said, there's ample improvements that can be made. The labeling and scoring in this prototype is still quite crude, and I'd love to increase the fidelity by crowdsourcing the scores with a more defined metric and methodology. The dataset of space needle images is fun, but quite specific. For this model to be truly useful to a photographer, I'd want to train a model on images more akin to the types of settingts they shoot in, and perhaps even add functionality to fine tune the model as a user applies the model to their data, learning the particulars of their taste and style. Additionally, there are many sorts of features that could be built on top of this architecture, such as capturing the model's latent vectors prior to classification and using them to identify images most similar to a selected picture. Always more to do!&lt;/p&gt;</content><category term="Python"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="CNN"></category><category term="Xception"></category><category term="Image Classification"></category><category term="AI"></category><category term="The Singularity"></category></entry><entry><title>Speakeasy, the AI Bartender</title><link href="http://scottbutters.com/articles/2019/05/27/Speakeasy-AI-Bartender/" rel="alternate"></link><published>2019-05-27T17:00:00-07:00</published><updated>2019-05-27T17:00:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:scottbutters.com,2019-05-27:/articles/2019/05/27/Speakeasy-AI-Bartender/</id><summary type="html">&lt;p&gt;You know that thing where you're hanging out in a schmancy speakeasy and the bartender asks you what you'd like to have—not in terms of a specific cocktail, or even the base spirit, but in terms of the flavor profile? And then just sets to work grabbing one bottle after another until before you know it you've got a little bit of magic in your mouth and you don't even know how? That. That right there is the epitome of mixology, as far as I'm concerned. That's "&lt;a href="http://speakeasy-ai-bartender.herokuapp.com/"&gt;the speakeasy experience&lt;/a&gt;." That's what I've sought to recreate with this app.&lt;/p&gt;</summary><content type="html">&lt;p&gt;You know that thing where you're hanging out in a schmancy speakeasy and the bartender asks you what you'd like to have—not in terms of a specific cocktail, or even the base spirit, but in terms of the flavor profile? And then just sets to work grabbing one bottle after another until before you know it you've got a little bit of magic in your mouth and you don't even know how? That. That right there is the epitome of mixology, as far as I'm concerned. That's "&lt;a href="http://speakeasy-ai-bartender.herokuapp.com/"&gt;the speakeasy experience&lt;/a&gt;." That's what I've sought to recreate with this app.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/bar.jpg" alt="speakeasy-bar" style="width:100%"&gt;
  &lt;figcaption&gt;My home bar, soon to be tended by my personal AI assistant.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://speakeasy-ai-bartender.herokuapp.com/"&gt;The SpeakEasy app&lt;/a&gt; is a cocktail recommendation engine that's built to transform plain English requests from a user into a suggested cocktail that best matches the request. The functionality of this system rests on three systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A database of cocktails, containing recipes, descriptions, and metadata for each.&lt;/li&gt;
&lt;li&gt;A model trained to vectorize and transform text describing a cocktail into an appropriate topic space.&lt;/li&gt;
&lt;li&gt;An app that uses the model trained on the cocktail database to make new predictions based on user's request.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first production version of the app is making use of cocktail data scraped from &lt;a href="https://www.diffordsguide.com/cocktails"&gt;Difford's Guide&lt;/a&gt;. The text data from each cocktail is processed into a document-term matrix using a TF-IDF vectorizer, then factored into a document (cocktail)-topic matrix using latent semantic analysis (LSA). Finally, the data, vectorizer and LSA transformation matrix are packaged into a Flask app and hosted on Heroku so all the world can get the drink they're itching for.&lt;/p&gt;
&lt;p&gt;The code for both the &lt;a href="https://github.com/BotScutters/SpeakEasy"&gt;SpeakEasy modeling&lt;/a&gt; and &lt;a href="https://github.com/BotScutters/speakeasy-app"&gt;SpeakEasy app&lt;/a&gt; are available on my &lt;a href="https://github.com/BotScutters"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Data Sources&lt;/h1&gt;
&lt;p&gt;Data! As much as with any other project, building this app demonstrated to me the value of high quality data, though this time with a particular flavor. That is, I started out using a dataset of cocktail recipes that were mostly terrible concoctions of flavored vodka and fruit juice. As they say, garbage juice in, garbage juice out. It wasn't until late in the model development that I decided I really ought to revisit step one and collect all new data. More on that later. &lt;/p&gt;
&lt;p&gt;In principle, though, in order for the recommendation algorithm to make decent suggestions given a variety of input types (i.e. the user might ask for a type of cocktail ("Manhattan variation"), a taste profile ("sweet and sour"), a combination ("spicy margarita"), or really any other thing) I needed a data source with not just a large number of high quality drinks, but also a substantial amount of text dedicated to describing the various qualities of the cocktails.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://drizly.com/recipes#all-recipes"&gt;Drizly&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;They've got a decent collection of about 600 cocktails, mostly pretty well described. I actually started out working with this dataset as my primary source. It's got two main downfalls, though: the formatting is somewhat inconsistent, making it impractical to extract recipes in a meaningful way, and more importantly, the cocktail list is padded with recipes that I'll snobbishly call garbage juice and seemed to give SpeakEasy a bias towards suggesting drinks that might glow under a blacklight, give you cancer, or both. No thanks. &lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.diffordsguide.com/cocktails"&gt;Difford's Guide&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Difford's Guide was my gold mine. Every cocktail on the site has been sampled, reviewed and rated by Mr. Difford himself, and from what I can tell the man's got standards. The drinks are good and interesting, the descriptions are generally decent for a NLP analysis, the website is laid out in a way that's relatively easy to scrape—a beautifully balanced data source for what I was looking for in this project. But the garnish that really tied the site together was that the recipe ingredients are all presented with consistent units and in a table format, making it exceedingly easy to extract the information and process it on the modeling end. Standardized units are the things of an engineer's dreams!&lt;/p&gt;
&lt;h2&gt;Future Data Sources&lt;/h2&gt;
&lt;p&gt;In the course of preparing this app I flagged a number of resources for cocktail recipes that would be fantastic to incorporate down the line. One of the biggest challenges in integrating them is simply the variations in website formatting and different types of available information (which could also be viewed as a strength, through a certain lens).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kindredcocktails.com/cocktail?scope=0"&gt;Kindred Cocktails&lt;/a&gt; - a strong cocktail database with lots of metadata to back it up. I briefly spoke with Dan, their Chief Swizzlestick, about a collaboration, but that seems to have fizzled out for now. Nonetheless, they're doing great work. Perhaps a relationship will be rekindled when I pick this back up.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.thespruceeats.com/a-to-z-cocktail-recipes-3962886"&gt;Spruce Eats&lt;/a&gt; - They've got a strong showing of about 1000 cocktails with pretty verbose descriptions. Just a matter of doing the work to incorporate.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1W59zl7F534rHhRhzxxAH0EMqbe4H7qGz646srCpRnXw/pub?single=true&amp;amp;gid=0&amp;amp;output=html"&gt;Imbibe + PDT Cocktails&lt;/a&gt; - a Reddit user put together this spreadsheet of cocktails. Lots of fantastic drinks, but sorely lacking in good descriptions. Would require a strong ingredient description library to cross reference and build-up the descriptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Tools&lt;/h2&gt;
&lt;h5&gt;Code&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Jupyter notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Data exploration and cleaning&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Regex&lt;/li&gt;
&lt;li&gt;Numpy&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Feature Preparation and NLP Modeling&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Gensim Doc2Vec&lt;/li&gt;
&lt;li&gt;Sklearn&lt;/li&gt;
&lt;li&gt;TfidfVectorizer&lt;/li&gt;
&lt;li&gt;TruncatedSVD&lt;/li&gt;
&lt;li&gt;cosine_similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Visualization&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;li&gt;Hvplot&lt;/li&gt;
&lt;li&gt;SkLearn TSNE&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Workflow management&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;DataScienceMVP&lt;/li&gt;
&lt;li&gt;Cookiecutter&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Acquisition&lt;/h2&gt;
&lt;p&gt;Scraping the cocktail data from Difford's Guide took two main steps: putting together a list of all of the cocktails I wanted to get data on (as well as URLs to each cocktail's page), and then actually going to the pages and getting the cocktail data.&lt;/p&gt;
&lt;h3&gt;Getting the List of Cocktails&lt;/h3&gt;
&lt;p&gt;To get my list of cocktails to scrape, I made use of the advanced search feature of the site. I found that if I conducted an advanced search to show me all cocktails with a rating of 3 stars or higher (gotta set the bar somewhere…), the page returned a total of well over 4,000 cocktails spread across over 100 pages. After a bit of investigation, I found that the URL carried all of the parameters and sending them through an internal API, including an offset parameter indicating which page to load. Jackpot! I wrote a function that generated URLs for each page, then proceeded to pull the source html from each page using Python's requests library and then parse the html with BeautifulSoup to extract the names and URLs of all displayed cocktails.&lt;/p&gt;
&lt;p&gt;Side note: an interesting hiccup I ran into while doing this is that I kept on coming up short of the expected number of cocktails I'd scrape from the page. After a bit of puzzling, I found that the cocktails were being displayed sorted by rating…but! Turns out that when cocktails had an identical rating, i.e. 100 different cocktails might have a 3.5 star rating, the order &lt;em&gt;within&lt;/em&gt; that rating group is (apparently) random. What's worse, the order seems to be calculated on every page load, since duplicates were regularly found on the following page. I ended up resolving this by having the list render sorted by name rather than rating.&lt;/p&gt;
&lt;h3&gt;Getting Data on Each Cocktail&lt;/h3&gt;
&lt;p&gt;I used two methods to get each individual cocktail's data. I started out by pulling the source html into BeautifulSoup and parsing it for various desired attributes, similar to above. However, one fun trick I employed was using the Pandas read_html function to do a lot of the parsing. The first table I pulled was the one containing each cocktail's recipe. I converted this directly into a DataFrame so I could easily extract different elements from the table, then converted the table &lt;em&gt;back&lt;/em&gt; into html using the to_html function. With only a little bit of clean-up and modification, this table is what I ultimately fed directly into the app later on to show a user the recipe! The other table I pulled is what contained the bulk of the text I used for each cocktail, i.e. description, garnish, instructions, etc. Because each cocktail might actually have a different number of entries in the table, pulling the table wholesale like this made the process much simpler to parse.&lt;/p&gt;
&lt;h2&gt;Data Preparation&lt;/h2&gt;
&lt;p&gt;Coming out of the acquisition stage I had all of my cocktail data effectively stored in JSON format. Before I could properly vectorize each cocktail, I needed to clean and combine the text for each cocktail into a single "document" or "bag of words". Basically what this meant was stepping through the data for each individual cocktail and appending all of the entries that seemed like they might be useful (descriptions, ingredients, etc.) into a single long string. Finally, I fed each string through a text cleaning function to remove extraneous html tags and unicode characters that managed to sneak their way in.&lt;/p&gt;
&lt;h2&gt;Modeling: LSA or Doc2Vec?&lt;/h2&gt;
&lt;p&gt;Oh, I spent a long time on this one, and frankly, I bet I'll spend more, as there's still more data to add and more algorithms to try out. But as of this writing I can say that I have fiercely pitted latent semantic analysis (LSA) against a Doc2Vec embedding and thus far LSA is winning. I'll come back to this.&lt;/p&gt;
&lt;p&gt;But from a high level, SpeakEasy behaves the same regardless of which method is in use:&lt;/p&gt;
&lt;p&gt;To train the model:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feed text from all cocktails in dataset into model and have model learn the overall vocabulary as well as various word frequencies within each cocktail description. The model is trained on this.&lt;/li&gt;
&lt;li&gt;Convert text of each cocktail description into a numerical vector representation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make a suggestion:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Receive user input in the form of a string.&lt;/li&gt;
&lt;li&gt;Using trained model, transform string into numerical vector.&lt;/li&gt;
&lt;li&gt;Suggest cocktail exhibiting the highest cosine similarity to this new vector.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that this method is making the assumption that (a) training data is robust, and that (b) our user input text contains relevant words that were represented in the original data. A bit obvious, but significant. A nonsense request will yield a nonsense suggestion. A request for a rare ingredient or unusual descriptor may well fall on deaf ears and, again, yield a nonsense suggestion.&lt;/p&gt;
&lt;p&gt;But now to the nitty gritty: how do we convert a string of text into numbers? The potential methods are many and the choice has a significant impact on what comes out the other side.&lt;/p&gt;
&lt;h3&gt;Latent Semantic Analysis&lt;/h3&gt;
&lt;p&gt;There are two main steps for implementing LSA: converting the text data to a numerical representation using &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;Sklearn's term frequency-inverse document frequency (TF-IDF) vectorization&lt;/a&gt;, and reducing the dimensionality of the newly created document-term matrix into a document-topic space using &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"&gt;Sklearn's TruncatedSVD&lt;/a&gt; function. This yields a topic-document matrix, which is what we ultimately compare the vector generated from user input against.&lt;/p&gt;
&lt;p&gt;There are a few choices to be made in tuning this model. &lt;/p&gt;
&lt;p&gt;For the vectorizer, I'm pre-processing the text by stripping all unicode accents, lowercasing all uppercase characters, and removing stop words (common words that contribute no meaning, such as "the", "and", etc.). I'm also creating additional tokens by including in my document-term matrix not just individual words, but also bigrams (all sets of two consecutive words from the processed text). There certainly exist further steps I could take that may improve model performance even further, including: stemming or lemmatizing tokens down to base forms, adding to or modifying the stock stop words than come built in to the library, adjusting frequency thresholds for which words make it into the model, and experimenting further with which ngrams to include.&lt;/p&gt;
&lt;p&gt;For the TruncatedSVD model, the primary choice to make was the number of vectors to reduce the document-term matrix down to. Evaluating this was tricky, but the method I ended up settling on was choosing a few test phrases to simulate user input and manually/subjectively measuring how good the suggestions were. Test phrases I tried were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Give me something sweet and smoky, like a margarita with mezcal"&lt;/li&gt;
&lt;li&gt;"can you make me a bitter manhattan variation with fernet?"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From there I looked at the different drinks suggested depending on the number of dimensions I was reducing to, and what types of features appeared to be captured by the algorithm.&lt;/p&gt;
&lt;p&gt;Ultimately what I found is that over approximately 100 dimensional vectors, though I was only capturing about 25% of the variance in the documents, there seemed to be diminishing returns and little change in the suggestions. I ended up settling at 128 dimensions per vector, though I fully expect that number to shift a bit as/when I add additional data to the model.&lt;/p&gt;
&lt;h3&gt;Doc2Vec&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://radimrehurek.com/gensim/models/doc2vec.html"&gt;Doc2Vec&lt;/a&gt; is a paragraph embedding model built on top of Word2Vec to give a numeric representation of an entire paragraph based on a continuous-bag-of-words model and a skip gram model. Effectively, Doc2Vec extends the functionality of Word2Vec from looking just at individual words and the few that surround them to looking at all of the words that make up a document and then producing a vector representation of that entire document.&lt;/p&gt;
&lt;p&gt;With this project, that means that Doc2Vec can potentially be a drop-in replacement for TF-IDF and Truncated SVD as described above. To train the model, we:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Preprocess each document, similar to how we did with LSA&lt;/li&gt;
&lt;li&gt;Instantiate a model with some hyperparameters&lt;/li&gt;
&lt;li&gt;Build a vocabulary of all tokens in document&lt;/li&gt;
&lt;li&gt;Train model on corpus &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make suggestions, we run the user input through the same processing engine, infer a vector for the new document, and return the cocktail with highest cosine similarity.&lt;/p&gt;
&lt;p&gt;But, I hear you asking, &lt;em&gt;what about those hyperparameters?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ah yes, the hyperparameters.&lt;/p&gt;
&lt;p&gt;So I wrote a grid search algorithm. Starting coarse and working my way toward an optimal configuration, I adjusted the values for number of dimensions per vector, number of epochs to train the model over, and the number of steps (epochs) to use for inferring the vector for test data. After much iteration, I landed on parameters of a vectorsize of 100, 400 epochs, and 3 steps. That should raise two big questions: wait, how'd you even decide what to call "good" in order to tune the model? And that aside, how'd you end up at 400 epochs when all the literature suggests you should be well below 100? &lt;/p&gt;
&lt;p&gt;I know. And it was a headache that I'm sure I'm not over with. But let's step through one at a time.&lt;/p&gt;
&lt;p&gt;For measuring goodness of fit, I decided to test how well the model would do at matching the text description of a cocktail &lt;em&gt;to itself&lt;/em&gt;. This is actually a pretty reasonable test with Doc2Vec, as the way it constructs the document vector is non-deterministic. That is to say, even after setting random seeds anywhere I could and even my Python hash seed, the model would &lt;em&gt;still&lt;/em&gt; infer a different (albeit similar) document vector for each document every time I ran it through. Though in practice I won't be feeding long description documents into the model, in theory I do want it to be fit well enough that when making a cocktail suggestion based on a document the model has been trained on, the correct answer is at least &lt;em&gt;close&lt;/em&gt; to the best match.&lt;/p&gt;
&lt;p&gt;So that was my parameter. For every configuration of hyperparameters that I tested, I ran each cocktail through the prediction function and took the average of all the rankings where I found the cocktail the test description had come from to be the error I wanted to minimize. This harrowing journey brought me to the parameters mentioned above, at which each cocktail would &lt;em&gt;on average&lt;/em&gt; be the predicted as the 6th best match with itself.&lt;/p&gt;
&lt;p&gt;As for the parameters: from what I've reae about Doc2Vec, I expected epochs to be happier closer to 20, and that I shouldn't need to specify the steps parameter at all—but these made a huge difference in performance. The vector size actually ended up around a reasonable value. A theory I have for &lt;em&gt;why&lt;/em&gt; the epochs seem so far off from what I'd expect is that my data may not actually fit the idea of a continuous bag of words all that well, given that adjacent words may sometimes be unrelated, if it's just a list of ingredients, and the topics jump around due to the way I combined the different fields from the website into a single document. Each document is not actually a continuous set of words so much as several groups of words bunched together.&lt;/p&gt;
&lt;h3&gt;Comparing the Two Methods&lt;/h3&gt;
&lt;p&gt;Gripes about Doc2Vec parameters aside, how'd they perform? For a final test, I basically replicated the procedure I used to tune the LSA model (though in hindsight I really should have tried manually tuning the Doc2Vec model this way; in the future I will). When comparing the output of each model to the same input, it was clear that LSA made suggestions that were far more relevant to the input request than what Doc2Vec was spitting out. So disappointing.&lt;/p&gt;
&lt;h2&gt;Packaging and Serving as an App&lt;/h2&gt;
&lt;p&gt;After finalizing the model I'd work with, it was time to package it all up and serve it in an app. To do this, I wrote an app using the Python's &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt; library along with Bootstrap for the templates and web styling and an API to pass the data back and forth between Javascript and Python. The app is now hosted &lt;a href="http://speakeasy-ai-bartender.herokuapp.com/"&gt;here&lt;/a&gt; using &lt;a href="www.heroku.com"&gt;Heroku&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An interesting (and ongoing) challenge I ran into while deploying the app was due to the file size. The upon pickling, the LSA model ends up being well over 100 MB, meaning that it can't be stored to and pulled from Github like the rest of the app. I resolved this by storing all of my pickled model files on Amazon S3, and adding a step to the app's startup sequence to download all of the models from AWS. While technically I could get away with compressing the LSA model to under 100 MB and then unzipping it upon loading the program, I think I will stick with the S3 method, as this will allow me to scale my dataset larger in the future.&lt;/p&gt;
&lt;p&gt;An additional problem, however, is that Heroku only grants 500 MB of RAM to an app unless you upgrade all the way to the third price tier. Not that that's an insane thing to do…but for a prototype? Not yet. So as of now, my app has been averaging a load of about 590 MB RAM, which means I'm receiving constant warnings that I'm above their threshhold. Fortunately, they're friendly enough to allow the app to keep running without erroring out completely. That said, I recognize SpeakEasy is in treacherous waters here and that I really ought to spend some time figuring out how to reduce the memory requirements of the app.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I've been dreaming about this app for a long time, and it's been hard work and a ton of fun bringing it to fruition. But…is SpeakEasy done? If you haven't picked up on it by now: SpeakEasy is a work in progress. There are bugs, sure, and I can see some flaws in the first steps I took. But also, I've got a ton of ideas about how I want to &lt;em&gt;improve&lt;/em&gt; SpeakEasy. On the laundry list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More data, always more data. Incorporate more cocktails from additional sources.&lt;/li&gt;
&lt;li&gt;Build a database of ingredients with descriptions of each of those. Cross-reference cocktail recipes with ingredient database to pad cocktail descriptions.&lt;/li&gt;
&lt;li&gt;Experiment with more methods of text preprocessing, primarily trying out stemming vs lemmatizing and doing some cross-sectional analysis to identify additional stop words to add. &lt;/li&gt;
&lt;li&gt;An additional interesting approach could be taking the text input and actually padding it with synonyms of the entered words. Could help to handle words that are similar but still different. &lt;/li&gt;
&lt;li&gt;Could also do some level of hierarchically classifying ingredients.&lt;/li&gt;
&lt;li&gt;Modeling methods! How well would the model perform if I used a probabilistic Latent Semantic Analysis (PSLA)? Or Latent Dirichlet Allocation (LDA) or Nonnegative Matrix Factorization (NMF)? What about going deeper down the embedding hole and trying lda2vec? Okay, but what if I go even deeper, all the way down the deep learning rabbit hole and implement some &lt;a href="https://arxiv.org/pdf/1707.07435.pdf"&gt;neural net magickry as described in this paper&lt;/a&gt;? Okay, I'm done now. Except not, I'll definitely obviously try all of these just as soon as I find all the time. I want to, at least.&lt;/li&gt;
&lt;li&gt;Develop a better testing method for evaluating goodness of model fit. Because you know what didn't feel good? My testing method, both for tuning hyperparameters and choosing a model. A better method and more quantifiable scoring metric would do worlds of good. Ideas? Let's talk!&lt;/li&gt;
&lt;li&gt;Improve user interface of app. Currently it's not exactly reliable in terms of uptime, rendering on different devices, and even predictions. &lt;/li&gt;
&lt;li&gt;Improve memory management of app. Even if I don't make drastic changes to the models, I could still switch to Gensim's implementation of Latent Semantic Indexing, which utilizes better memory management (constant size, not impacted by size of corpus) as well as the ability to update the model with streaming data (bonus!).&lt;/li&gt;
&lt;li&gt;Add logging for user requests and suggested matches (believe it or not, I'm not tracking you), and provide user with the ability to up or downvote the recommendation they received. Long term it would be awesome to incorporate that into the suggestions, but short term I could even use it for guiding choices about model development.&lt;/li&gt;
&lt;li&gt;Build sister app, the AI Budtender. I've already done a bunch of the legwork, as described in my previous blog about scraping dispensary data off of &lt;a href="http://localhost:8000/articles/2019/04/29/scraping-leafy/"&gt;Leafly.com&lt;/a&gt;. Extending the code I wrote there to get data on all of the different strains wouldn't be a very big stretch, and SpeakEasy would pretty happily train on that dataset without so much as a "No way, man." &lt;/li&gt;
&lt;li&gt;Thirteenth but not last, INVENT COCKTAILS! This is the problem I'm most excited to work on, but it's…bigger. I want to develop the algorithm that won't just suggest a cocktail near to a description but actually invent the recipe that matches it best—and to it well. There are a lot of problems baked into that question, but man do I want to figure out how to answer them.&lt;/li&gt;
&lt;li&gt;This isn't over.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading and feel free to reach out with questions or comments!&lt;/p&gt;</content><category term="Python"></category><category term="Data Science"></category><category term="NLP"></category><category term="SVD"></category><category term="LSA"></category><category term="Cocktails"></category><category term="Recommender"></category><category term="Flask"></category><category term="App"></category><category term="Robots"></category><category term="AI"></category><category term="The Singularity"></category></entry><entry><title>SpeakEasy, the AI bartender: it's alive!</title><link href="http://scottbutters.com/articles/2019/05/23/the-SpeakEasy-app-is-alive/" rel="alternate"></link><published>2019-05-23T00:32:00-07:00</published><updated>2019-05-27T17:00:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:scottbutters.com,2019-05-23:/articles/2019/05/23/the-SpeakEasy-app-is-alive/</id><summary type="html">&lt;p&gt;I wrote an app that uses NLP and really cool math to suggest cocktails!&lt;/p&gt;</summary><content type="html">&lt;p&gt;This one'll be short and sweet. For the past few weeks I've been consumed by a project to build an AI bot that emulates my favorite kind of experience with a bartender. You know that thing where you're hanging out in a schmancy speakeasy and the bartender asks you what you'd like to have—not in terms of a specific cocktail, or even liquor, but in terms of the flavor profile? And then just sets to work grabbing one bottle after another until before you know it you've got a little bit of magic in your mouth and you don't even know how? That. That right there is the epitome of mixology, as far as I'm concerned. That's "the speakeasy experience." That's what I've sought to recreate with this app. Am I done? No, of course not! But I've got a working prototype out and exposed to the world and I'm too excited to keep quiet about it. So &lt;a href="http://speakeasy-ai-bartender.herokuapp.com/"&gt;check it out&lt;/a&gt;, and head on over to &lt;a href="http://scottbutters.com/articles/2019/05/27/Speakeasy-AI-Bartender/"&gt;this post&lt;/a&gt; if you want to dive into the nitty-gritty of how it works under the hood and what kinds of plans I've got for the future of SpeakEasy! &lt;/p&gt;</content><category term="Python"></category><category term="Data Science"></category><category term="NLP"></category><category term="SVD"></category><category term="LSA"></category><category term="Cocktails"></category><category term="Recommender"></category><category term="Flask"></category><category term="App"></category><category term="Robots"></category><category term="AI"></category><category term="The Singularity"></category></entry><entry><title>Using Machine Learning to Predict Flight Cancellations</title><link href="http://scottbutters.com/articles/2019/05/08/predicting-flight-cancellations/" rel="alternate"></link><published>2019-05-08T19:35:00-07:00</published><updated>2019-05-08T19:35:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:scottbutters.com,2019-05-08:/articles/2019/05/08/predicting-flight-cancellations/</id><summary type="html">&lt;p&gt;Over 99% of scheduled domestic flights take off. That remaining 1% is very expensive, however, by some estimates costing the airline industry nearly a billion dollars every year. Can we use machine learning to predict these cancellations in advance to reduce the associated losses?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Over 99% of all scheduled domestic flights take off, and the rest are very expensive. Of over 10 million scheduled flights each year, hundreds of thousands never leave the tarmac. At an average of $5,770 in losses per cancellation this adds up to nearly a billion dollars of opportunity. While a significant chunk of those losses is unavoidable, it stands to reason that if an airline could confidently predict a cancellation far enough in advance, they could start taking measures to reduce those losses by doing such things as rebooking stranded passengers and rescheduling flight crews.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/cancelled_flight.jpg" alt="cancelled-flight" style="width:100%"&gt;
  &lt;figcaption&gt;A sight best left unseen&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;p&gt;The aim of this project is to build a model capable of predicting flight cancellations at least 24 hours in advance of the scheduled departure time. The premise behind this is that within sufficient foresight of an impending cancellation, an airline can take preemptive measures to reduce the ensuing cost and misery for all involved. To do this, I constructed a dataset of over 12 years of flight records and weather data for the same period and trained machine learning algorithms to predict future flight cancellations based on these historical observations. While this article will give a broad overview of the project and touch on some of the more interesting problems within, you can also see a full walkthrough of the code and model development in &lt;a href="https://github.com/BotScutters/predicting-flight-cancellations/blob/master/notebooks/mvp.ipynb"&gt;this notebook&lt;/a&gt; on my Github.&lt;/p&gt;
&lt;h1&gt;Data Sources&lt;/h1&gt;
&lt;h2&gt;Flight Records&lt;/h2&gt;
&lt;p&gt;My data set is primarily derived from the "&lt;a href="https://www.transtats.bts.gov/Tables.asp?DB_ID=120&amp;amp;DB_Name=Airline%20On-Time%20Performance%20Data&amp;amp;DB_Short_Name=On-Time#"&gt;Marketing Carrier On-Time Performance&lt;/a&gt;" database hosted by the Bureau of Transportation Statistics, which contains information on nearly every flight conducted by a significant U.S. carrier dating back to January 1987. As that amount of data is pretty unwieldy for a prototype, for the purposes of this project I've narrowed the scope to only making predictions on flights departing from my home airport, the Seattle-Tacoma International Airport, from 2007 to the end of 2018.&lt;/p&gt;
&lt;h2&gt;Weather Records&lt;/h2&gt;
&lt;p&gt;To supplement the flight data, the model also makes use of historical weather data for the same time duration. This weather data is acquired via the Dark Sky API, and gives hourly records of precipitation, temperature, wind, visibility, and more. I couldn't, unfortunately, find a resource for historical weather &lt;em&gt;forecasts&lt;/em&gt;, so I ended up using a combination of day-before weather observations and constructing a proxy for forecasts by adding noise to actual observations from the day of flights.&lt;/p&gt;
&lt;h2&gt;Future Data Sources&lt;/h2&gt;
&lt;p&gt;There are a number of additional features that this model would likely benefit from that have not yet been incorporated. Likely future resources include aircraft specific features that can be acquired by cross referencing with &lt;a href="https://www.faa.gov/licenses_certificates/aircraft_certification/aircraft_registry/releasable_aircraft_download/"&gt;this database&lt;/a&gt; from the FAA.&lt;/p&gt;
&lt;h2&gt;Tools&lt;/h2&gt;
&lt;h5&gt;Data Management&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Postgresql&lt;/li&gt;
&lt;li&gt;Sqlalchemy&lt;/li&gt;
&lt;li&gt;Psycopg2&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Code&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Jupyter notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Data exploration and cleaning&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Numpy&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Feature Preparation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Sklearn&lt;/li&gt;
&lt;li&gt;Standard scaler&lt;/li&gt;
&lt;li&gt;PCA&lt;/li&gt;
&lt;li&gt;Select K Best&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Modeling&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Sklearn&lt;/li&gt;
&lt;li&gt;Logistic regression&lt;/li&gt;
&lt;li&gt;K nearest neighbors&lt;/li&gt;
&lt;li&gt;Gaussian Naive Bayes&lt;/li&gt;
&lt;li&gt;Support vector machine with linear kernel&lt;/li&gt;
&lt;li&gt;Support vector machine with radial basis function&lt;/li&gt;
&lt;li&gt;Random forest &lt;/li&gt;
&lt;li&gt;Gradient boosting&lt;/li&gt;
&lt;li&gt;Adaptive boosting&lt;/li&gt;
&lt;li&gt;Multi-layer perceptron with linear activation function&lt;/li&gt;
&lt;li&gt;Multi-layer perceptron with RELU activation function&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Visualization&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;li&gt;Seaborn&lt;/li&gt;
&lt;li&gt;Hvplot&lt;/li&gt;
&lt;li&gt;Tableau&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Workflow management&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;DataScienceMVP&lt;/li&gt;
&lt;li&gt;Cookiecutter&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Cleaning, Combining, and Developing the Data&lt;/h2&gt;
&lt;p&gt;Just acquiring the flight data for this project was a pretty significant task—over 50 GB of csvs. To handle such a time- and space-intensive dataset, I wrote a script to step through each individual file, strip away all of the data that wouldn't be useful for machine learning, then load the remaining data into a SQL database.   Reducing this down to only data I would use for training my models, which is all flights scheduled to depart from Seattle on or after January 1, 2006, this still left me with data on about 1.5 million flights.&lt;/p&gt;
&lt;p&gt;Of course, how could one predict flight cancellations without taking into account the weather? This, however, was much easier to acquire. To acquire all of the weather data, I signed up for a very affordable API key from Dark Sky and pulled hourly weather observations for every day I cared to predict on and loaded all of this into another table in my SQL database.&lt;/p&gt;
&lt;h3&gt;Feature Engineering&lt;/h3&gt;
&lt;h4&gt;Flight Data&lt;/h4&gt;
&lt;p&gt;Now it was time to turn all of this data into something our algorithms can understand. While there's a wealth of data about each flight, very little of it is actually numeric, which is what we really need. To address this, I devised a number of ways to slice the data into different interpretations of cancellation rates. These include things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;overall cancellation rate over the past 7 days&lt;/li&gt;
&lt;li&gt;cancellation rate by a given airline over the past 7 days&lt;/li&gt;
&lt;li&gt;cancellation rate for this specific aircraft over the past 7 days&lt;/li&gt;
&lt;li&gt;cancellation rate for all flights going to a particular destination over the past 7 days&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And of course more variations along those lines, with additional grouping features as well as different rolling average and fixed time spans. &lt;/p&gt;
&lt;h4&gt;Weather Data&lt;/h4&gt;
&lt;p&gt;Parsing the weather data was quite a bit simpler. In my initial calls to the API I loaded a handful of numeric features into my database, such as temperature, cloud cover, precipitation, visibility, and a few more. Since the weather data came in hourly increments and the flight data comes with a feature chunking flight times into hourly blocks (other than a large block from midnight to 6 am, for some godawful reason), I chose to join the weather features with the flight data using the date and departure time blocks as a composite key.&lt;/p&gt;
&lt;p&gt;The biggest hiccup, as I mentioned before, was the lack of historical &lt;em&gt;forecast&lt;/em&gt; data, only historical observations. Since I wanted my model to be making predictions using only data that would be available to airlines 24 hours in advance of a flight, this called for a little bit of creativity. Modern weather forecasting is generally quite good, especially only a day in advance in the area around a major airport, so I decided to construct some features to serve as a proxy for actual forecasts. To do this, I generated a matrix of Gaussian noise centered around 1 with a standard deviation of 0.025 and multiplied this element-wise against a copy of my weather data. This resulted in a table of weather forecasts where about 95% of the values were within 5% of the actual observed values, approximating the accuracy of our actual weather forecasting systems.&lt;/p&gt;
&lt;h2&gt;Modeling and Cross-Validation&lt;/h2&gt;
&lt;p&gt;With a design matrix in place, it was time to split my data into a handful of training and testing sets for cross-validation along with a holdout dataset for final testing. Since this is a time-series prediction, I needed to ensure that there would be no data leakage from my testing sets into my training sets, so chose to break down the data into a series of rolling windows with 6-year long training sets and always predicting on the following year—in a production setting you would likely retrain significantly more often, but for this prototype that method was time and compute-prohibitive.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/flight_CV.png" alt="flight_CV" style="width:100%"&gt;
  &lt;figcaption&gt;Structure of cross-validation and holdout train-test splits&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3&gt;Modeling&lt;/h3&gt;
&lt;p&gt;It was nearly time to start training models—but which models to train? Rather than make that decision upfront, I built my modeling pipeline to flexibly accept a variety of classification models and parameters from sklearn's toolbox and planned to iteratively test these with my cross-validation sets and gradually eliminate models as I saw fit. Included in my testing were the following classifiers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logistic regression&lt;/li&gt;
&lt;li&gt;K nearest neighbors&lt;/li&gt;
&lt;li&gt;Naive Bayes&lt;/li&gt;
&lt;li&gt;Support vector machine with linear kernel&lt;/li&gt;
&lt;li&gt;Support vector machine with radial basis function&lt;/li&gt;
&lt;li&gt;Random forest &lt;/li&gt;
&lt;li&gt;Gradient boosting&lt;/li&gt;
&lt;li&gt;Adaptive boosting&lt;/li&gt;
&lt;li&gt;Multi-layer perceptron with linear activation function&lt;/li&gt;
&lt;li&gt;Multi-layer perceptron with RELU activation function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Can I start teaching the robots now?&lt;/p&gt;
&lt;h3&gt;Data Processing&lt;/h3&gt;
&lt;p&gt;While my design matrix at this point was technically sufficient for modeling—that is, it was entirely numerical and had no null values—it still had a number of issues making it sub-optimal for modeling.&lt;/p&gt;
&lt;h4&gt;Inconsistent scaling&lt;/h4&gt;
&lt;p&gt;With features describing cancellation rates, precipitation, temperature, and more, the scales of my data were all over the place. To handle this, I used sklearn Standard Scaler module to rescale all of my features to be centered around a mean of 0 with a standard deviation of 1.&lt;/p&gt;
&lt;h4&gt;Too many features with not enough information&lt;/h4&gt;
&lt;p&gt;The model contained about 60 different features, many of them exhibiting high multicollinearity. Many of these were in fact constructions from the same data and similar-but-different interpretations of the same questions. To handle this, I applied two steps in series (though it took quite a bit of cross-validation for me to decided exactly how). &lt;/p&gt;
&lt;h5&gt;PCA&lt;/h5&gt;
&lt;p&gt;The first step I took was to apply principal component analysis to the data to reduce the dimensionality from 60 features down to 20 features. The theory behind this is that it should effectively collapse and combine my most similar features into new composite features that will capture the majority of the variance in the data in a more condensed form. While this technically results in a loss of resolution in the data, it also creates richer data more dense with information and proved to be more help than harm in my modeling.&lt;/p&gt;
&lt;p&gt;It should be noted that while using PCA was helpful for condensing my features, a major downside to this action is that it severely reduces the interpretability of the model, as once the transformation has been applied it's no longer apparent which features are contributing most to a model's prediction.&lt;/p&gt;
&lt;h5&gt;Select K Best&lt;/h5&gt;
&lt;p&gt;The next step was to reduce that set of 20 features down to only those that conveyed the most information about whether or not a flight would be cancelled. To do this I utilized the SelectKBest module within sklearn and the f_classif function to compute the ANOVA F-value for each feature to determine the most predictive features to feed into the model. Ultimately after cross-validation I settled on using 12 features, but by no means did this seem to be a "clear winner".&lt;/p&gt;
&lt;h4&gt;Oversampling to restore balance&lt;/h4&gt;
&lt;p&gt;The dataset itself was highly imbalanced with respect to my target variable of cancellations. That is to say that since 99 out of every 100 flights isn't cancelled, a model could simply predict that a flight won't be cancelled every time and be 99% accurate while not actually doing anything useful. There are three ways to respond to an imbalanced dataset like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Do nothing.&lt;/li&gt;
&lt;li&gt;Oversample the minority class until it's better represented.&lt;/li&gt;
&lt;li&gt;Undersample the majority class to bring the two into parity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a lazy human, option 1 sure was tempting. But as an overachiever, I couldn't help but consider the others. I quickly observed that more data was better, so option 3 was out. That left option 2. I experimented with oversampling methods like SMOTE and ADASYN, but pretty quickly saw a fear of my come to fruition: by overrepresenting cancellations in my training set, I had trained my models to expect them far too often. This resulted in a very high number of false positives, which in this particular scenario is the worst case scenario. So I went back to step 1 and decided to do nothing about my imbalanced set.&lt;/p&gt;
&lt;h3&gt;Custom Score Function&lt;/h3&gt;
&lt;p&gt;Nearly ready to run my models, I found myself facing a critical question I'd been kicking around but putting off throughout the entire development process. How do I &lt;em&gt;judge&lt;/em&gt; my models? While there are many standard responses out there for an imbalanced dataset (don't use accuracy, it's a trap!), how was I to know which one to use? I could use the &lt;a href="https://en.wikipedia.org/wiki/F1_score"&gt;F1 score&lt;/a&gt;, but giving equal weight to both precision and recall without a good reason feels awfully arbitrary. I could seek to maximize the area under the curve of my &lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;ROC&lt;/a&gt; score, but again, that's so divorced from the actual question I'm asking…so I did the natural thing and devised my own scoring metric.&lt;/p&gt;
&lt;p&gt;I decided assign a cost/benefit value to each possible outcome of a prediction based on actual estimates of what these decisions would mean to an airline if they put the model into production. There are only four possible scenarios so…not too tough. Let me walk you through the reasoning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True Negative: The model correctly predicted that a flight that actually took off wouldn't be cancelled. An airline using the model wouldn't behave any differently in this circumstance, so this outcome incurs no effect on the score.&lt;/li&gt;
&lt;li&gt;False Negative (&lt;em&gt;FN&lt;/em&gt;): The model incorrectly predicted that a flight that was actually cancelled would take off. An airline using the model wouldn't behave any differently in this circumstance, so even though the outcome is less than ideal, it incurs no effect on the score. &lt;/li&gt;
&lt;li&gt;False Positive: The model incorrectly predicted that a flight that actually took off would be cancelled. An airline using the model would mistakenly cancel the flight, incurring a heavy penalty cost, $\gamma_0$ which I'll elaborate on below.&lt;/li&gt;
&lt;li&gt;True Positive: The model correctly predicts that a flight that was actually cancelled would be cancelled. This is the holy grail, the real opportunity for savings. To quantify this, I made an assumption that with sufficient foreknowledge an airline could begin taking measures that would allow it to save some proportion of the expected losses due to cancellation, $\gamma_1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That might seem a bit long winded, but don't worry: it gets a bit worse before it gets better. I threw out those gammas to represent cost and benefit, but what are those really? I did a bit of research and number crunching to make some best guess estimates, and in a nutshell it looks a little like this. Based on data from &lt;a href="https://247wallst.com/services/2014/10/29/the-high-cost-of-cancelled-airline-flights/"&gt;this site&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On average, a cancellation costs an airline about $5,770&lt;/li&gt;
&lt;li&gt;That can be narrowed down to about $3,205 for flights canceled due to an uncontrollable event like weather and more like \$9,615 if the cancellation was due to something the airline should have been able to control, such as missing flight crew or maintenance.&lt;/li&gt;
&lt;li&gt;What that means is that about 45% of costs incurred due to cancellations &lt;em&gt;were avoidable&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;This is extra money that goes towards things like inefficient usage of flight and maintenance crews as well as rebooking/reimbursing disgruntled passengers&lt;/li&gt;
&lt;li&gt;This means that on average about $2600 of the money lost due to a cancelled flight is money that the airline didn't have to lose. That's our margin to save from.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On a similar note, taking some averages from a variety of resources, we can estimate that an average flight has about 100 passengers on board and makes about $18 of profit per passenger. In other words, the average profit per flight is about \$1800.&lt;/p&gt;
&lt;p&gt;Finally, there's that dangling $\gamma_1$ variable I threw out there. This is a wild assumption that would have to be manipulated in a production setting, and it's worth noting that all of my extrapolated interpretations hinge on this value. That said, I had to choose something. So what's it mean? I decided that a conservative estimate was that with sufficient foresight, an airline could maybe take back about 10% of that $2600 recoverable margin I mentioned above. In reality that might be 5 or 50, I'm not sure—but I went with 10.&lt;/p&gt;
&lt;p&gt;With that in place, we can compute our cost of a false positive, which is the combination of lost profit plus the additional losses incurred by a cancelled flight (albeit a more efficiently cancelled flight, since we did it in advance). Skipping over the algebra, I can just tell you that it reduces to $\gamma_0 = \gamma_1 - \frac {profit} {margin} $.&lt;/p&gt;
&lt;p&gt;To turn all of that into a usable score function, we add up the cost/benefit of every prediction in a test run and then finally normalize it against the total number of actual cancellations multiplied by our cost savings parameter, $\gamma_1$, like so:&lt;/p&gt;
&lt;p&gt;$$ Score = \frac {\sum [\gamma_0 (FP) + \gamma_1 (TP)]} { \gamma_1 * (FN + TP)} $$&lt;/p&gt;
&lt;p&gt;What this yields is a score where 0 means that the ultimate result would be that the airline sees no change in revenue, a negative number indicates that the airline would actually lose money by adopting the model, and a positive number (bounded at 1) represents the proportion of total possible savings the airline would be harnessing (i.e. a true positive rate of 1 would result in a score of 1).&lt;/p&gt;
&lt;h2&gt;Interpreting the Results&lt;/h2&gt;
&lt;p&gt;Okay, that was a mouthful, but also somewhat necessary. But now we can interpret our models! So how'd we do? It's quite tempting to describe the results as so-so, and that's not entirely wrong—but it's not entirely right either, so let's dig in.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/model_performance.png" alt="model_performance" style="width:100%"&gt;
  &lt;figcaption&gt;Performance of various models on the data, with CV tests faded and the final test highlighted.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The chart above deserves a bit of interpretation. All the faded dots in the background represent average cross-validation test scores my models achieved with different parameters and inputs, and the highlighted result is the test score from the final run on my holdout dataset. Arguably, a jump from 0.25 down to 0.08 suggests that the model didn't generalize too well to 2018. And I don't thing that's wrong. So let's dive into a couple of these observations.&lt;/p&gt;
&lt;p&gt;A point worth noticing is that pretty much all of the models seemed to plateau right around 0.25. Curious, eh? What that suggests to me is if there's a fundamental issue with the predictions it's not the models that are at fault but the features. It seems that the data on hand was able to predict about 25% of the reasons why flights would be canceled leading up to 2017, and that &lt;em&gt;all&lt;/em&gt; of the models were able to identify those particular flights. However, it also seems that those other 75% of flights weren't being predicted at all, by any of the models, suggesting a lack of critical features. Furthermore, the fact that predictions on 2018 dropped so significantly suggest that &lt;em&gt;something changed&lt;/em&gt; that the model couldn't keep up with.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The final result of a score 0.08 isn't stellar, but I think it's worth noting that it's not actually terrible either. Putting it into real world terms, this number means that my model correctly predicted 8% of all of SeaTac's flight cancellations in the year 2018 a full 24 hours before they happened. What's more, the model did this without making a single false positive prediction, which means that if airlines were utilizing the model, there would be zero mistaken cancellations and room for an estimated $12,000 in savings. Not an amount I'd get too excited about, but if you extrapolate that out to every airport in the US, now we're looking at more like a potential for around \$2 million dollars in savings per year. Not so shabby after all.&lt;/p&gt;
&lt;h2&gt;Future Work&lt;/h2&gt;
&lt;p&gt;This project was a fun prototype, but the conclusion certainly leaves a bit to be desired. That said, there's lots of room for improvement. Some obvious avenues to go down that I think would yield significant results include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Incorporating airplane-specific features, such as make/model and maintenance history&lt;/li&gt;
&lt;li&gt;Improved weather forecasting features&lt;/li&gt;
&lt;li&gt;Tailoring the cost function to take into account size of aircraft and number of passengers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading and feel free to reach out with questions or comments!&lt;/p&gt;</content><category term="SQL"></category><category term="Python"></category><category term="Data Science"></category><category term="Machine Learning"></category><category term="Classification"></category><category term="Neural Net"></category><category term="Time-Series"></category></entry><entry><title>Scraping Leafly.com for Data on the Marijuana Industry</title><link href="http://scottbutters.com/articles/2019/04/29/scraping-leafy/" rel="alternate"></link><published>2019-04-29T16:20:00-07:00</published><updated>2019-04-29T16:20:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:scottbutters.com,2019-04-29:/articles/2019/04/29/scraping-leafy/</id><summary type="html">&lt;p&gt;Leafly is an information aggregator for cannabis. They maintain a profile for most of the dispensaries in the state. This makes them a pretty valuable resource for studies on the cannabis industry. Here's how I scraped Leafly for ratings, reviews, inventory counts, and other metadata on every dispensary in the state of Washington.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Leafly is an information aggregator for cannabis. They maintain a profile for most of the dispensaries in the state. In a project to study and make predictions about &lt;a href="http://scottbutters.com/articles/2019/04/20/mary-jane-model/"&gt;what drives the Washington state cannabis industry&lt;/a&gt;, I've scraped the following features from the Leafly website for each dispensary for which it was available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average customer rating and number of customer reviews&lt;/li&gt;
&lt;li&gt;Inventory counts (number of products under descriptions like "flower", "edibles", "concentrates", etc.&lt;/li&gt;
&lt;li&gt;Categorical qualities, such as whether or not the store is ADA accessible or has an ATM onsite&lt;/li&gt;
&lt;li&gt;Metadata such as name, address, phone number, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The combination of these features gives us a profile of each dispensary that allow us to draw insights into what makes for a successful dispensary.&lt;/p&gt;
&lt;h2&gt;Obtaining the Data&lt;/h2&gt;
&lt;h3&gt;Scraping Leafly&lt;/h3&gt;
&lt;p&gt;Getting data I was looking for off of from Leafly's website was by far the most difficult and complex task in the course of this prediction project. To tackle this, I split up the task into two primary functions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting a list of all of the dispensaries with a profile on Leafly, along with some basic metadata about each (most importantly, the URL suffix that points to the dispensary's profile page)&lt;/li&gt;
&lt;li&gt;Going to each individual dispensary profile and extracting specific data of interest available there.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Getting a list of all of the dispensaries&lt;/h4&gt;
&lt;p&gt;In a dream world there would be a single page somewhere on Leafly pointing to each of the 500 or so dispensaries in Washington, making it trivial to extract all those links and continue on to get all my data and accomplish great things. This is not that world. Instead, Leafly has this lovely interface of a map view and tiles, dynamically rendered and updated as you move the map or search a new area. Unfortunately, a given map view doesn't actually render all of the dispensaries in that view, nor does the URL interface allow for the map to be easily searched in a programmatic way. Modern javascript allows for some beautiful web design, but it wasn't giving me any handouts here.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/leafly-disp-search.png" alt="leafly-disp-search" style="width:100%"&gt;
  &lt;figcaption&gt;Dispensary search page on Leafly.com&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;That's where a little bit of network traffic sleuthing comes in. It turns out that if you dig into the calls and requests made by the site while the page is loading you can find a searchThisArea API call being made to render the webpage, the results of which include data on every dispensary within a rectangle described by latitude and longitude coordinates. While Leafly doesn't technically offer a public facing API, I was able to exploit this request URL to get what I needed. I wrote a grid search function to systematically enter lat/lon coordinates that would traverse the entire state of Washington one little box at a time.&lt;/p&gt;
&lt;p&gt;From here the solution was mostly just a straightforward process of converting request responses to JSON and storing my desired data to a dictionary, I do want to talk briefly about what I thought was a clever response to the API return limit of 200 results per search. Backing up a little bit, in order to avoid being detected as an automated scraping algorithm and having my IP blacklisted from Leafly, my algorithm would wait a random amount of time (anywhere from 0.5-2.5 seconds) between each request in order to obfuscate the fact that it's a data-sucking robot. The downside of this for me is that the smaller my search area in my grid search, the longer it would take me to scrape the whole state.&lt;/p&gt;
&lt;p&gt;My solution to that was to implement what approximately amounts to a recursive tree search of the state that tends to roughly minimize the number of API calls required to collect all of the data. I initialized the search by instructing my algorithm to search the entire state of Washington. This invariably returns data on exactly 200 dispensaries, as that's where the API limits the call—problematic when what's desired is &lt;em&gt;all&lt;/em&gt; of the dispensary. So I set up my routine such that the first thing it does is check whether or not the number of responses equals 200. If so, it simply subdivides the search area into four smaller search areas and searches them by the same routine. This has the effect of automatically determining an appropriately zoomed in search area when moving over dense areas like Seattle while searching from zoomed way out while moving over the more rural parts of the state.&lt;/p&gt;
&lt;h4&gt;Getting dispensary specific data&lt;/h4&gt;
&lt;p&gt;So that was fun, and by the end of the routine I had a data dictionary with entries for a little over 600 dispensaries (my coordinates overlaps into Oregon a bit, and from the looks of it the overachieving stoners down in Portland out-consume Seattle by about 2:1).&lt;/p&gt;
&lt;p&gt;From here, I had a new scraping algorithm to write. To get the data off of each individual dispensary page, I used Selenium to fire up an instance of Chrome to be programmatically driven to each page and select and scrape the needed information. Of course, the first step in the process was to have my robobrowser tell Leafly that it was indeed over 21 years of age.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="http://scottbutters.com/img/are-you-21.png" alt="are-you-21" style="width:60%"&gt;
  &lt;figcaption&gt;Minor defense on Leafly.com&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Simple enough, just had to tell the robot to find the Yes button and click it, and…success! My robot successfully thwarted their robot's defense perimeter and we're off to the races. On each page, I pulled every bit of data I could find that might be relevant, from continuous variables like ratings, number of reviews, and quantity of each product found to be in stock to the categorical things like whether or not they had an ATM on site or are ADA accessible.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/lux-pot-shop.png" alt="lux-pot-shop" style="width:100%"&gt;
  &lt;figcaption&gt;Dispensary profile page on Leafly.com&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And that was that! With a couple dozen fields of data on each dispensary now stashed away in a JSON file, I could move on to scraping some simpler sources of data. &lt;/p&gt;
&lt;h2&gt;Using the data&lt;/h2&gt;
&lt;p&gt;If you enjoyed this, check out my article on how I used this data to make predictions about &lt;a href="http://scottbutters.com/articles/2019/04/20/mary-jane-model/"&gt;what gets the Washington cannabis industry high&lt;/a&gt;.&lt;/p&gt;</content><category term="Python"></category><category term="Data Science"></category><category term="Web Scraping"></category><category term="Leafly"></category><category term="Marijuana"></category><category term="Washington"></category></entry><entry><title>Mary Jane, the Model: Using Machine Learning to Predict Marijuana Dispensary Performance</title><link href="http://scottbutters.com/articles/2019/04/20/mary-jane-model/" rel="alternate"></link><published>2019-04-20T14:15:00-07:00</published><updated>2019-04-21T12:10:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:scottbutters.com,2019-04-20:/articles/2019/04/20/mary-jane-model/</id><summary type="html">&lt;p&gt;In 2012, Washington state passed I-502 and legalized the recreational sale, use, and possession of marijuana. This event has led to an explosion of development in the field that's making waves through our society. Since 2014, approximately 500 state licensed dispensaries have opened throughout the state, with nearly 150 of those here in Seattle. In this project I scour the web for publicly available data that might be predictive of how a cannabis dispensary performs, such as customer reviews, inventory distributions, and local demographics. I then train machine learning models to predict a dispensary's monthly revenue and analyze the resulting models to distill insights about what drives sales in the marijuana market.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Marijuana is a controversial subject, to be sure. That said, as Washington state and others have legalized recreational pot over the past handful of years the industry has gone from being confined to the shadows to operating on full display in showrooms reminiscent of Apple stores and luxury car dealerships. &lt;/p&gt;
&lt;figure&gt;
  &lt;img src="http://scottbutters.com/img/budtender-1024x640.jpg" alt="budtender" style="width:100%"&gt;
  &lt;figcaption&gt;Budtender Austin Tucker at Dockside Cannabis in SODO, courtesy of Leafly&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the last two weeks, I've been immersed in a cloud of data surrounding the Washington marijuana industry, attempting to model the relationship between factors like online reviews and local demographics and a dispensary's reported revenue. While it's been a bit hazy at times, it's ultimately led to some enlightening insights. I'd love to share with you some of what I've learned while dabbling with Mary Jane, the model. &lt;/p&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In 2012, Washington state passed I-502 and legalized the recreational sale, use, and possession of marijuana. This has led to  an explosion of development in the field that's making waves through our society. Since 2014, approximately 500 state licensed dispensaries have opened throughout the state, with nearly 150 of those here in Seattle. In this project, I scoured the web for publicly available data that might be predictive of how a cannabis dispensary performs, such as customer reviews, inventory distributions, and local demographics. I then trained machine learning models to predict a dispensary's monthly revenue and analyze the resulting models to distill insights about what drives sales in the marijuana market. For the full source code from my project, check out &lt;a href="https://github.com/BotScutters/dispensary-predictions"&gt;my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Data Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Washington State Liquor and Cannabis Board (WSLCB)&lt;/li&gt;
&lt;li&gt;Leafly&lt;/li&gt;
&lt;li&gt;WA HomeTownLocator&lt;/li&gt;
&lt;li&gt;Walkscore&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Licensing and Sales Data from &lt;a href="https://lcb.wa.gov/"&gt;WSLCB&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Lastly, all that data would get us nowhere if we didn't have any target data to train our models on. That's where the WSLCB comes in. The WSLCB maintains data on every dispensary in the state, including monthly reports of revenue (which is what our model is predicting). Their data is scattered across a couple of different outlets, but for this project I used spreadsheets downloadable from &lt;a href="https://lcb.wa.gov/records/frequently-requested-lists"&gt;this obscure page&lt;/a&gt; to get sales data dating back to November 2017. Because the only identifying information in that spreadsheet is the license number of the dispensary, I also downloaded a spreadsheet listing metadata for every entity that has applied for a Marijuana license, which I then joined with the sales data in order to link it up with data scraped from other resources.&lt;/p&gt;
&lt;h3&gt;Dispensary profiles from &lt;a href="www.leafly.com"&gt;Leafly&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Leafly is an information aggregator for cannabis. They maintain a profile for most of the dispensaries in the state. As part of my dataset, I've scraped the following features from the Leafly website for each dispensary for which it was available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average customer rating and number of customer reviews&lt;/li&gt;
&lt;li&gt;Inventory counts (number of products under descriptions like "flower", "edibles", "concentrates", etc.&lt;/li&gt;
&lt;li&gt;Categorical qualities, such as whether or not the store is ADA accessible or has an ATM onsite&lt;/li&gt;
&lt;li&gt;Metadata such as name, address, phone number, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The combination of these features gives us a profile of each dispensary that allow us to draw insights from our model into what makes for a successful dispensary.&lt;/p&gt;
&lt;h3&gt;Demographics from &lt;a href="https://washington.hometownlocator.com/"&gt;WA HomeTownLocator&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Of course, having the best inventory, friendliest staff and prettiest pot shop in the state doesn't amount to anything if a dispensary is in the middle of nowhere. This is where demographic data comes in. WA HomeTownLocator maintains a database of demographic statistics for nearly every zip code in the state of Washington. The data is produced by Esri Demographics, and updated 4 times per year using data from the federal census, IRS, USPS, as well as local data sources and more. From this website I scraped data likely to be predictive of a local market such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Population density&lt;/li&gt;
&lt;li&gt;Diversity&lt;/li&gt;
&lt;li&gt;Average income&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These data give our model an image of what a dispensary's customer base is like, allowing us to characterize what makes for a good location to establish a dispensary.&lt;/p&gt;
&lt;h3&gt;&lt;a href="www.walkscore.com"&gt;Walkscore&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I also used the Walkscore API to query their database for scores on how easily consumers can reach each dispensary by foot or bike.&lt;/p&gt;
&lt;h2&gt;Tools&lt;/h2&gt;
&lt;h5&gt;Code&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Jupyter notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Data exploration and cleaning&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Numpy&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;Fuzzywuzzy&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Modeling&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Sklearn&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Visualization&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;li&gt;Seaborn&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Web scraping&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Requests&lt;/li&gt;
&lt;li&gt;Selenium&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Workflow management&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;DataScienceMVP&lt;/li&gt;
&lt;li&gt;Cookiecutter&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Obtaining the Data&lt;/h2&gt;
&lt;p&gt;The methods for acquiring the data for this project really varied significantly depending upon the source. Here I'll talk briefly about the methods I used for each source and touch on the difficulties and learning experiences I had with each.&lt;/p&gt;
&lt;h3&gt;WSLCB License and Sales Data&lt;/h3&gt;
&lt;p&gt;Other than actually finding this data, this was arguably the easiest piece of data to acquire for this project. If you go down the path of searching for dispensary sales data in the obvious place (the state's &lt;a href="https://data.lcb.wa.gov/browse"&gt;Marijuana Dashboard&lt;/a&gt;, which hosts a bunch of cool interactive charting tools to browse the data the various data that's been collected related to the marijuana industry), you'll be disappointed to discover that they basically stopped updating these stats on that page in 2017. No, it turns out that you instead need to go to the WSLCB's &lt;a href="https://lcb.wa.gov/records/frequently-requested-lists"&gt;Frequently Requested Lists&lt;/a&gt; page, which has download links to a hodge podge of datasets. Meandering path aside, it's now just a matter of simply downloading two spreadsheets—one containing metadata for every marijuana license holder in the state (names, addresses, etc.) and the other containing monthly sales numbers for each license holder dating back to November 2017. My code automatically searches the page for the latest versions and updates that into the training pipeline.&lt;/p&gt;
&lt;h3&gt;Scraping Leafly&lt;/h3&gt;
&lt;p&gt;Getting data I was looking for off of from Leafly's website was by far the most difficult and complex task in acquiring data for this project. To tackle this, I split up the task into two primary functions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting a list of all of the dispensaries with a profile on Leafly, along with some basic metadata about each (most importantly, the URL suffix that points to the dispensary's profile page)&lt;/li&gt;
&lt;li&gt;Going to each individual dispensary profile and extracting specific data of interest available there.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It was really a project in and of itself, so check up my more detailed write-up on that &lt;a href="http://scottbutters.com/articles/2019/04/29/scraping-leafly/"&gt;here&lt;/a&gt;. With a couple dozen fields of data on each dispensary now stashed away in a JSON file, I could move on to scraping some simpler sources of data.&lt;/p&gt;
&lt;h3&gt;Scraping demographic data&lt;/h3&gt;
&lt;p&gt;Getting demographic data for each dispensary area was actually a bit more challenging than I expected. While US census data is publicly available, it's generally not provided at a very granular or intuitive level. After a good long time searching around different aggregator sites, I came across the Washington Hometown Locator website, which offers up data down to each zip code in the state.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="http://scottbutters.com/img/demo-data.png" alt="demo-data" style="width:80%"&gt;
  &lt;figcaption&gt;Zip code level demographic data from Washington Hometown Locator&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;From here, I wrote a simple script that would:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Extract every zip code from my dataset of dispensary license holders.&lt;/li&gt;
&lt;li&gt;Generate a URL using each of these zip codes to point to the appropriate page on WHL&lt;/li&gt;
&lt;li&gt;Download the table data from each of these pages using the Pandas function pd.read_html. It was actually kind of miraculous to me how simple and effective this method was. Thanks to &lt;a href="https://datatostories.com/"&gt;Young Jeong&lt;/a&gt; for the tip!&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Getting walk and bike scores from the Walkscore API&lt;/h3&gt;
&lt;p&gt;It seemed plausible to me that I could leverage &lt;a href="https://www.walkscore.com/"&gt;Walkscore&lt;/a&gt;'s scoring of how prime a location is according to how walkable it is, and fortunately for me it they've got a public facing API that's free to access (up to a 5000 requests a day). This one was actually as simple as applying for an API key (took about an hour to get it) and writing a script to generate request call URLs by combining address, city, state, zip code, and the lat/lon coordinates for each location. The only complication is that I didn't yet actually have any single data source where providing me with all of this information. For that small reason I actually don't execute this step until late in the code after I've already done quite a bit of cleaning and merging of the data.&lt;/p&gt;
&lt;h2&gt;Cleaning, Combining, and Developing the Data&lt;/h2&gt;
&lt;h3&gt;Merging data without a common key&lt;/h3&gt;
&lt;h3&gt;Determining dispensary density&lt;/h3&gt;
&lt;h2&gt;Exploration and Feature Engineering&lt;/h2&gt;
&lt;p&gt;Some of my favorite tools for exploring relationships in data are built right in to the Seaborn plotting library. Let me walk through some quick examples of how I use jointplot, heatmap, and pairplots to identify trends in the data and give me clues on how I ought to transform my features.&lt;/p&gt;
&lt;h3&gt;A jointplot for each feature vs target variable combination&lt;/h3&gt;
&lt;p&gt;Jointplots are a pretty slick wrapper on the JointGrid class where with just a quick switch of a parameter you can get a variety of plot types (scatter, regression, residual, kde or hex) that display a nice amount of information. For my initial explorations I used jointplot to display a scatterplot with univariate regression line for every feature compared to my target variable, target_sales, along with kernel density histograms for each variable. An example and the code to produce it are shown below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jointplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;number_of_reviews&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total_sales&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1750&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1300000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;reg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;joint_kws&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;line_kws&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;color&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;scatter_kws&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;}})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;figure align="middle"&gt;
  &lt;img src="http://scottbutters.com/img/jointplot.svg" alt="jointplot" style="width:80%"&gt;
  &lt;figcaption&gt;Jointplot showing univariate regression on # of reviews vs total sales with kernel density histograms for each variable&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There's quite a bit we can learn from just a quick glance at this plot (and many others like it not produced here). For starters, that best fit line doesn't look like it fits very well. It seems we've got the majority of the data all scrunched up in the corner and a cloud of outliers fanning out in a rather thin fashion from there. A look at the histograms confirms that the data is quite skewed. Given that the variables we're working with are both counts that are bounded at 0, this is unsurprising. Fortunately, we've got tools to handle this sort of thing. I estimated that this feature (and many others) along with the target variable grow in an exponential fashion, meaning that I might be able to find some more linear relationships if I apply a log transform. Next step: create logarithmically transformed columns for every feature (and the target variable) exhibiting this trend. Check out the shift in this column once I applied that change:&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="http://scottbutters.com/img/log_jointplot.svg" alt="jointplot" style="width:80%"&gt;
  &lt;figcaption&gt;Jointplot showing univariate regression on log(# of reviews) vs log(total sales) with kernel density histograms for each variable&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Is it a beautiful fit? Well, no, not exactly. The number_of_reviews feature is clearly a little bit broken, what with all those 0 values there. But I'd argue that our best fit line looks much more reasonable now than before, and our distributions are substantially closer to a Gaussian normal (which is definitely something we want). At this stage, I make a note to myself that maybe I can improve this feature more, but let's work with it as is for now.&lt;/p&gt;
&lt;h3&gt;Heatmap of correlations&lt;/h3&gt;
&lt;p&gt;So how about a different view of our data? I don't know about you, but I'm a big fan of heatmaps. Like I might be a little bit &lt;em&gt;too&lt;/em&gt; into em. Ever since that time in college when I got really into darts and decided to track all of my throws to figure out my &lt;a href="https://amloceanographic.com/wp-content/uploads/2017/08/Bullseye-Accuracy-vs-Precision-1024x602.jpg"&gt;accuracy/precision profile&lt;/a&gt; looked like… but I digress. What was I saying? Heatmaps! They're a fantastic way to check out two important qualities of your features: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Which ones are most correlated with your target variable?&lt;/li&gt;
&lt;li&gt;Which ones exhibit high multicollinearity with each other?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Producing the following plot is fabulously simple (other than the particulars of formatting things prettily, which always manage to be a pain). But the basics are as follows.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Filter the df down to strictly continuous numeric variables&lt;/span&gt;
&lt;span class="n"&gt;numeric&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_dtypes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Produce a correlation matrix sorted according to the target variable&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numeric&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corr&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Heatmap! In shades of green, because context matters&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;heatmap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;BuGn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;figure align="middle"&gt;
  &lt;img src="http://scottbutters.com/img/heatmap.svg" alt="heatmap" style="width:100%"&gt;
  &lt;figcaption&gt;Correlation map of a selection of features and the target variable&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And no, this isn't &lt;em&gt;all&lt;/em&gt; of my features—but if you plot too many, rendering and interpretability becomes a real issue. But looking at just these, there's a few trends we can pick out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The features (shown here) exhibiting the highest correlations with the log transform of total sales are the log transforms of the number of reviews, population density, and the number of dispensaries within 10 miles&lt;/li&gt;
&lt;li&gt;There are several categories of features which exhibit moderately high multicollinearity. These tend to be clustered according to their data source. i.e. a dispensary with a large "All Products" count tends to also have large numbers of prerolls and flowers in stock. An area with high per capita income also has high median and average home values.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These things aren't surprising, but they are important. We'll want to make sure to account for that multicollinearity later, as too much of it leads to not so great regressions, as the model can't decide which features to focus on.&lt;/p&gt;
&lt;h2&gt;Building and Refining a Model&lt;/h2&gt;
&lt;p&gt;I got a little bit a head of myself there, though. It's very tempting to run down the rabbit hole of feature engineering ad infinitum, but it's also important to just get your model working. So before I started transforming all my features, that's what I &lt;em&gt;actually&lt;/em&gt; did. Let's step a few minutes back in time and walk through that process. I'm sorry, I really am, this iterative stuff just isn't terribly…linear.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;five hours earlier&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Building an MVP (Minimum Viable Product) with multivariate linear regression&lt;/h3&gt;
&lt;p&gt;Once I had all my data combined and formatted into a &lt;a href="https://en.wikipedia.org/wiki/Design_matrix"&gt;design matrix&lt;/a&gt;, it was time to run my first linear regression and get an idea of my baseline model performance. Mind you, the purpose here is not yet to have a particularly &lt;em&gt;good&lt;/em&gt; model, but simply to have established a simple, functioning pipeline that we can quickly and easily iterate on. &lt;/p&gt;
&lt;p&gt;Of course, I couldn't simply allow myself to just throw &lt;em&gt;all&lt;/em&gt; of my features into a linear regression model and call that good—at this point I was looking at 60 continuous variables, many of them highly covariant, and had just shy of 400 observations in my data. A ratio like that is a certain recipe for an unstable solutionspace that overfits every time, and I wouldn't wish it even on the model of my worst enemy. So I ran a quick &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_top_corrs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total_sales&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Given dataframe, return a list of the n features most strongly correlated &lt;/span&gt;
&lt;span class="sd"&gt;    with the target variable&lt;/span&gt;
&lt;span class="sd"&gt;    Input: data in a Pandas dataframe, int for how many features desired&lt;/span&gt;
&lt;span class="sd"&gt;    (default 15), string of column name representing target variable&lt;/span&gt;
&lt;span class="sd"&gt;    Output: features, a list of strings representing the column names of just &lt;/span&gt;
&lt;span class="sd"&gt;    the top n features&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;numeric_corrs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_dtypes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corr&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;top_corrs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numeric_corrs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_corrs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_processed_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_top_corrs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total_sales&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and Voila! I had myself a dataframe containing only my 15 features most highly correlated with my target variable. Sure, 15 was an arbitrary choice that just felt a bit better than 60 and I knew there were all kinds of imperfections in the data…but this was something I could regress on with a bit less guilt.&lt;/p&gt;
&lt;p&gt;From there I generated an 80/20 train/test split (with a fixed random seed for repeatability, of course), fit a vanilla linear regression model on the X, y values from my training set and then used the model to predict the y values based on only the X from my training set. Unfortunately the precise results from my first run have been lost to the iterative ages, but if memory serves my initial training R^2 came out to about 0.3, with my test R^2 a bit closer to 0.2. I stuck the last few steps in a for-loop and iterated through 50 random seeds to generate a plot much like the following to show me how much it was varying just based on how the dataset was randomly split. Note that the plot below was actually generated after I had already started a bit of feature engineering.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="http://scottbutters.com/img/mvp.png" alt="mvp" style="width:60%"&gt;
  &lt;figcaption&gt;Scoring metrics from initial regression run prior to data processing and feature engineering&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;At this point I could only loosely argue that my model was performing better than if had simply predicted the mean dispensary revenue every time. Plenty of room to improve!&lt;/p&gt;
&lt;h3&gt;Optimizing our model and feature selection with feature scaling and regularization through lasso regression&lt;/h3&gt;
&lt;p&gt;This is the point where I entered the iterative phase, which basically meant living in a feedback loop bouncing back and forth between model experimentation and feature engineering—until it didn't seem like I was going to get performance much better within the time constraints at hand. &lt;/p&gt;
&lt;p&gt;To do this, I wrote a pipeline function to perform the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Import processed data into a Pandas dataframe&lt;/li&gt;
&lt;li&gt;Scale each feature by subtracting off its mean value and dividing it by its standard deviation.&lt;/li&gt;
&lt;li&gt;Split the dataframe ("randomly") into a training set containing 80% of the observations and a test set containing the other 20%.&lt;/li&gt;
&lt;li&gt;Stash the 20% test set far far away where my model couldn't see it.&lt;/li&gt;
&lt;li&gt;Instantiate a regression model through sci-kits learn. I used:&lt;/li&gt;
&lt;li&gt;Linear regression&lt;/li&gt;
&lt;li&gt;Ridge regression (with alpha parameter)&lt;/li&gt;
&lt;li&gt;Lasso regression (with alpha parameter)&lt;/li&gt;
&lt;li&gt;Perform a 5-way cross validation split on the training data (so splitting it further into 5 groups, each containing 80/20 split of the 80% training set.&lt;/li&gt;
&lt;li&gt;Train the model on each cross-validation training set, then compute error metrics using the cv test sets.&lt;/li&gt;
&lt;li&gt;Take the average error over the set of 5 runs and log that in a report to be referenced later when comparing models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that framework in place, I proceeded to subject my data to a battery of experimental models. Because I knew that I have way too many features for such limited data, I decided to try out ridge and lasso regression regularization methods as their cost functions are effective for reducing feature count and suppressing multicollinearity, which I certainly had plenty of. &lt;/p&gt;
&lt;p&gt;Though sklearn has functions in place (RidgeCV and LassoCV) that automatically optimize the penalty hyperparameter ${\lambda}$ (or $\alpha$ within sklearn) through cross validation, I decided to recode the process by hand. Because like they say, reinventing the wheel is the best way to know your way around it. I mean, I bet &lt;em&gt;somebody's&lt;/em&gt; set that before.&lt;/p&gt;
&lt;p&gt;So I set up a for loop to iteratively run models on the data while tweaking the value for $\lambda$ on a logarithmic scale until I'd identified the range of values that tended to minimize the mean squared error of the model when run on the test data. From there I modified my range and zoomed in until I'd identified values that seemed "optimal enough" (90.658 for ridge regression and 0.0172 for lasso). It was time to make a choice. The results of final runs with each model are copied below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Rows&lt;/th&gt;
&lt;th&gt;Cols&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Train R^2&lt;/th&gt;
&lt;th&gt;Test R^2&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Ridge regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.5026&lt;/td&gt;
&lt;td&gt;0.3439&lt;/td&gt;
&lt;td&gt;0.0977&lt;/td&gt;
&lt;td&gt;0.3126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.4676&lt;/td&gt;
&lt;td&gt;0.3782&lt;/td&gt;
&lt;td&gt;0.0931&lt;/td&gt;
&lt;td&gt;0.3051&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Their performance was close, but ultimately I chose the lasso regression as my preferred model for this situation both because it performed better in terms of my error metrics an also because it has the effect of actually zeroing out the coefficients on underperforming features rather than just suppressing them to low values. This feature strikes me as just a touch better generalization and more easily interpretable, too.&lt;/p&gt;
&lt;p&gt;Final model selected, I went ahead and dug up my true test data from the vault and ran a final prediction on the as-yet-unseen test data.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Rows&lt;/th&gt;
&lt;th&gt;Cols&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Train R^2&lt;/th&gt;
&lt;th&gt;Test R^2&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Ridge regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.5026&lt;/td&gt;
&lt;td&gt;0.3439&lt;/td&gt;
&lt;td&gt;0.0977&lt;/td&gt;
&lt;td&gt;0.3126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.4676&lt;/td&gt;
&lt;td&gt;0.3782&lt;/td&gt;
&lt;td&gt;0.0931&lt;/td&gt;
&lt;td&gt;0.3051&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Final lasso regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.4546&lt;/td&gt;
&lt;td&gt;0.3781&lt;/td&gt;
&lt;td&gt;0.1094&lt;/td&gt;
&lt;td&gt;0.3307&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Slightly worse than our cross validated results, that's to be expected. While we've reached a model that is certainly more robust than our original regression, our performance is still lackluster. There's obviously plenty of room for improvement if this ever gets picked up again.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="http://scottbutters.com/img/residuals.png" alt="residuals" style="width:80%"&gt;
  &lt;figcaption&gt;Scatterplot of final model performance, where green is true values of test data observations, red is model predictions, and blue is the residuals (error) of each prediction.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2&gt;So What Insights Can We Glean?&lt;/h2&gt;
&lt;p&gt;misc, number_of_reviews, population_in_group_qrtrs, population_density&lt;/p&gt;
&lt;h3&gt;Descaling and translating our coefficients into terms we can understand&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/th&gt;
&lt;th align="right"&gt;&lt;strong&gt;Scaled Coefficients&lt;/strong&gt;&lt;/th&gt;
&lt;th align="right"&gt;&lt;strong&gt;Unscaled Coefficients&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;log_number_of_reviews&lt;/td&gt;
&lt;td align="right"&gt;0.15479356&lt;/td&gt;
&lt;td align="right"&gt;0.22254295&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;log_population_density&lt;/td&gt;
&lt;td align="right"&gt;0.07910167&lt;/td&gt;
&lt;td align="right"&gt;0.09257050&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;prerolls&lt;/td&gt;
&lt;td align="right"&gt;0.02178097&lt;/td&gt;
&lt;td align="right"&gt;0.00023536&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;owner_occupied_housing_units_#&lt;/td&gt;
&lt;td align="right"&gt;0.01376879&lt;/td&gt;
&lt;td align="right"&gt;0.00000379&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;all_products&lt;/td&gt;
&lt;td align="right"&gt;0.01267409&lt;/td&gt;
&lt;td align="right"&gt;0.00002623&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;log_population_in_group_qrtrs&lt;/td&gt;
&lt;td align="right"&gt;0.01068558&lt;/td&gt;
&lt;td align="right"&gt;0.01412068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;number_of_reviews&lt;/td&gt;
&lt;td align="right"&gt;0.00977085&lt;/td&gt;
&lt;td align="right"&gt;0.00007235&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;per_capita_income&lt;/td&gt;
&lt;td align="right"&gt;0.00622086&lt;/td&gt;
&lt;td align="right"&gt;0.00000056&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;misc&lt;/td&gt;
&lt;td align="right"&gt;0.00205136&lt;/td&gt;
&lt;td align="right"&gt;0.00001326&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Future Work&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Collect more data&lt;/li&gt;
&lt;li&gt;More extensive exploratory data analysis&lt;/li&gt;
&lt;li&gt;Time series projections&lt;/li&gt;
&lt;li&gt;Look at revenue &lt;em&gt;growth&lt;/em&gt; as a target variable&lt;/li&gt;
&lt;li&gt;Experiment with different models&lt;/li&gt;
&lt;li&gt;Suggest optimal locations and product lines&lt;/li&gt;
&lt;/ul&gt;</content><category term="Python"></category><category term="Data Science"></category><category term="Web Scraping"></category><category term="Leafly"></category><category term="Marijuana"></category><category term="Washington"></category></entry><entry><title>Welcome to Bits 'n' Bots!</title><link href="http://scottbutters.com/articles/2019/04/20/my-first-blog-post/" rel="alternate"></link><published>2019-04-20T12:38:00-07:00</published><updated>2019-04-20T12:57:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:scottbutters.com,2019-04-20:/articles/2019/04/20/my-first-blog-post/</id><summary type="html">&lt;p&gt;Introduction to what the bits 'n' bots blog is about&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Bits 'n' Bots: Alive?&lt;/h1&gt;
&lt;p&gt;Welcome to Bits 'n' Bots, the up and coming repo of my thoughts, musings, projects and passions. Here we'll find a incomplete catalog of the off-the-wall topics I obsess over and explore. This blog will be generally data centric and analytical, a site to host and discuss my data science explorations, but from time to time I expect I'll also venture into topics such as futurist ethics, robotics, and really just about anything related to the impending singularity. Thanks for reading and feel free to reach out if any of my work catches your interest!&lt;/p&gt;</content><category term="Python"></category><category term="Data Science"></category><category term="Robots"></category><category term="AI"></category><category term="The Singularity"></category></entry></feed>