<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bits 'n' Bots</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2019-04-21T12:10:00-07:00</updated><entry><title>Mary Jane, the Model: Using Machine Learning to Predict Marijuana Dispensary Performance</title><link href="/articles/2019/04/20/mary-jane-model/" rel="alternate"></link><published>2019-04-20T14:15:00-07:00</published><updated>2019-04-21T12:10:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:None,2019-04-20:/articles/2019/04/20/mary-jane-model/</id><summary type="html">&lt;p&gt;In 2012, Washington state passed I-502 and legalized the recreational sale, use, and possession of marijuana. This event has led to an explosion of development in the field that's making waves through our society. Since 2014, approximately 500 state licensed dispensaries have opened throughout the state, with nearly 150 of those here in Seattle. In this project I scour the web for publicly available data that might be predictive of how a cannabis dispensary performs, such as customer reviews, inventory distributions, and local demographics. I then train machine learning models to predict a dispensary's monthly revenue and analyze the resulting models to distill insights about what drives sales in the marijuana market.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Marijuana is a controversial subject, to be sure. That said, as Washington state and others have legalized recreational pot over the past handful of years the industry has gone from being confined to the shadows to operating on full display in showrooms reminiscent of Apple stores and luxury car dealerships. &lt;/p&gt;
&lt;figure&gt;
  &lt;img src="/img/budtender-1024x640.jpg" alt="budtender" style="width:100%"&gt;
  &lt;figcaption&gt;Budtender Austin Tucker at Dockside Cannabis in SODO, courtesy of Leafly&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the last two weeks, I've been immersed in a cloud of data surrounding the Washington marijuana industry, attempting to model the relationship between factors like online reviews and local demographics and a dispensary's reported revenue. While it's been a bit hazy at times, it's ultimately led to some enlightening insights. I'd love to share with you some of what I've learned while dabbling with Mary Jane, the model. &lt;/p&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In 2012, Washington state passed I-502 and legalized the recreational sale, use, and possession of marijuana. This has led to  an explosion of development in the field that's making waves through our society. Since 2014, approximately 500 state licensed dispensaries have opened throughout the state, with nearly 150 of those here in Seattle. In this project, I scoured the web for publicly available data that might be predictive of how a cannabis dispensary performs, such as customer reviews, inventory distributions, and local demographics. I then trained machine learning models to predict a dispensary's monthly revenue and analyze the resulting models to distill insights about what drives sales in the marijuana market. For the full source code from my project, check out &lt;a href="https://github.com/BotScutters/dispensary-predictions"&gt;my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Tools&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Jupyter notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data exploration and cleaning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Numpy&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;Fuzzywuzzy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Modeling&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sklearn&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;li&gt;Seaborn&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Web scraping&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requests&lt;/li&gt;
&lt;li&gt;Selenium&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Workflow management&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DataScienceMVP&lt;/li&gt;
&lt;li&gt;Cookiecutter&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Washington State Liquor and Cannabis Board (WSLCB)&lt;/li&gt;
&lt;li&gt;Leafly&lt;/li&gt;
&lt;li&gt;WA HomeTownLocator&lt;/li&gt;
&lt;li&gt;Walkscore&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Licensing and Sales Data from &lt;a href="https://lcb.wa.gov/"&gt;WSLCB&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Lastly, all that data would get us nowhere if we didn't have any target data to train our models on. That's where the WSLCB comes in. The WSLCB maintains data on every dispensary in the state, including monthly reports of revenue (which is what our model is predicting). Their data is scattered across a couple of different outlets, but for this project I used spreadsheets downloadable from &lt;a href="https://lcb.wa.gov/records/frequently-requested-lists"&gt;this obscure page&lt;/a&gt; to get sales data dating back to November 2017. Because the only identifying information in that spreadsheet is the license number of the dispensary, I also downloaded a spreadsheet listing metadata for every entity that has applied for a Marijuana license, which I then joined with the sales data in order to link it up with data scraped from other resources.&lt;/p&gt;
&lt;h3&gt;Dispensary profiles from &lt;a href="www.leafly.com"&gt;Leafly&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Leafly is an information aggregator for cannabis. They maintain a profile for most of the dispensaries in the state. As part of my dataset, I've scraped the following features from the Leafly website for each dispensary for which it was available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average customer rating and number of customer reviews&lt;/li&gt;
&lt;li&gt;Inventory counts (number of products under descriptions like "flower", "edibles", "concentrates", etc.&lt;/li&gt;
&lt;li&gt;Categorical qualities, such as whether or not the store is ADA accessible or has an ATM onsite&lt;/li&gt;
&lt;li&gt;Metadata such as name, address, phone number, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The combination of these features gives us a profile of each dispensary that allow us to draw insights from our model into what makes for a successful dispensary.&lt;/p&gt;
&lt;h3&gt;Demographics from &lt;a href="https://washington.hometownlocator.com/"&gt;WA HomeTownLocator&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Of course, having the best inventory, friendliest staff and prettiest pot shop in the state doesn't amount to anything if a dispensary is in the middle of nowhere. This is where demographic data comes in. WA HomeTownLocator maintains a database of demographic statistics for nearly every zip code in the state of Washington. The data is produced by Esri Demographics, and updated 4 times per year using data from the federal census, IRS, USPS, as well as local data sources and more. From this website I scraped data likely to be predictive of a local market such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Population density&lt;/li&gt;
&lt;li&gt;Diversity&lt;/li&gt;
&lt;li&gt;Average income&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These data give our model an image of what a dispensary's customer base is like, allowing us to characterize what makes for a good location to establish a dispensary.&lt;/p&gt;
&lt;h3&gt;&lt;a href="www.walkscore.com"&gt;Walkscore&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I also used the Walkscore API to query their database for scores on how easily consumers can reach each dispensary by foot or bike.&lt;/p&gt;
&lt;h2&gt;Obtaining the Data&lt;/h2&gt;
&lt;p&gt;The methods for acquiring the data for this project really varied significantly depending upon the source. Here I'll talk briefly about the methods I used for each source and touch on the difficulties and learning experiences I had with each.&lt;/p&gt;
&lt;h3&gt;WSLCB License and Sales Data&lt;/h3&gt;
&lt;p&gt;Other than actually finding this data, this was arguably the easiest piece of data to acquire for this project. If you go down the path of searching for dispensary sales data in the obvious place (the state's &lt;a href="https://data.lcb.wa.gov/browse"&gt;Marijuana Dashboard&lt;/a&gt;, which hosts a bunch of cool interactive charting tools to browse the data the various data that's been collected related to the marijuana industry), you'll be disappointed to discover that they basically stopped updating these stats on that page in 2017. No, it turns out that you instead need to go to the WSLCB's &lt;a href="https://lcb.wa.gov/records/frequently-requested-lists"&gt;Frequently Requested Lists&lt;/a&gt; page, which has download links to a hodge podge of datasets. Meandering path aside, it's now just a matter of simply downloading two spreadsheetsâ€”one containing metadata for every marijuana license holder in the state (names, addresses, etc.) and the other containing monthly sales numbers for each license holder dating back to November 2017. My code automatically searches the page for the latest versions and updates that into the training pipeline.&lt;/p&gt;
&lt;h3&gt;Scraping Leafly&lt;/h3&gt;
&lt;p&gt;Getting data I was looking for off of from Leafly's website was by far the most difficult and complex task in the data acquisition portion. To tackle this, I split up the task into two primary functions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting a list of all of the dispensaries with a profile on Leafly, along with some basic metadata about each (most importantly, the URL suffix that points to the dispensary's profile page)&lt;/li&gt;
&lt;li&gt;Going to each individual dispensary profile and extracting specific data of interest available there.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Getting a list of all of the dispensaries&lt;/h4&gt;
&lt;p&gt;In a dream world there would be a single page somewhere on Leafly pointing to each of the 500 or so dispensaries in Washington, making it trivial to extract all those links and continue on to get all my data and accomplish great things. This is not that world. Instead, Leafly has this lovely interface of a map view and tiles, dynamically rendered and updated as you move the map or search a new area. Unfortunately, a given map view doesn't actually render all of the dispensaries in that view, nor does the URL interface allow for the map to be easily searched in a programmatic way. Modern javascript allows for some beautiful web design, but it wasn't giving me any handouts here.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="/img/leafly-disp-search.png" alt="leafly-disp-search" style="width:100%"&gt;
  &lt;figcaption&gt;Dispensary search page on Leafly.com&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;That's where a little bit of network traffic sleuthing comes in. It turns out that if you dig into the calls and requests made by the site while the page is loading you can find a searchThisArea API call being made to render the webpage, the results of which include data on every dispensary within a rectangle described by latitude and longitude coordinates. While Leafly doesn't technically offer a public facing API, I was able to exploit this request URL to get what I needed. I wrote a grid search function to systematically enter lat/lon coordinates that would traverse the entire state of Washington one little box at a time.&lt;/p&gt;
&lt;p&gt;From here the solution was mostly just a straightforward process of converting request responses to JSON and storing my desired data to a dictionary, I do want to talk briefly about what I thought was a clever response to the API return limit of 200 results per search. Backing up a little bit, in order to avoid being detected as an automated scraping algorithm and having my IP blacklisted from Leafly, my algorithm would wait a random amount of time (anywhere from 0.5-2.5 seconds) between each request in order to obfuscate the fact that it's a data-sucking robot. The downside of this for me is that the smaller my search area in my grid search, the longer it would take me to scrape the whole state.&lt;/p&gt;
&lt;p&gt;My solution to that was to implement what approximately amounts to a recursive tree search of the state that tends to roughly minimize the number of API calls required to collect all of the data. I initialized the search by instructing my algorithm to search the entire state of Washington. This invariably returns data on exactly 200 dispensaries, as that's where the API limits the callâ€”problematic when what's desired is &lt;em&gt;all&lt;/em&gt; of the dispensary. So I set up my routine such that the first thing it does is check whether or not the number of responses equals 200. If so, it simply subdivides the search area into four smaller search areas and searches them by the same routine. This has the effect of automatically determining an appropriately zoomed in search area when moving over dense areas like Seattle while searching from zoomed way out while moving over the more rural parts of the state.&lt;/p&gt;
&lt;h4&gt;Getting dispensary specific data&lt;/h4&gt;
&lt;p&gt;So that was fun, and by the end of the routine I had a data dictionary with entries for a little over 600 dispensaries (my coordinates overlaps into Oregon a bit, and from the looks of it the overachieving stoners down in Portland out-consume Seattle by about 2:1).&lt;/p&gt;
&lt;p&gt;From here, I had a new scraping algorithm to write. To get the data off of each individual dispensary page, I used Selenium to fire up an instance of Chrome to be programmatically driven to each page and select and scrape the needed information. Of course, the first step in the process was to have my robobrowser tell Leafly that it was indeed over 21 years of age.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="/img/are-you-21.png" alt="are-you-21" style="width:60%"&gt;
  &lt;figcaption&gt;Minor defense on Leafly.com&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Simple enough, just had to tell the robot to find the Yes button and click it, andâ€¦success! My robot successfully thwarted their robot's defense perimeter and we're off to the races. On each page, I pulled every bit of data I could find that might be relevant, from continuous variables like ratings, number of reviews, and quantity of each product found to be in stock to the categorical things like whether or not they had an ATM on site or are ADA accessible.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="/img/lux-pot-shop.png" alt="lux-pot-shop" style="width:100%"&gt;
  &lt;figcaption&gt;Dispensary profile page on Leafly.com&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And that was that! With a couple dozen fields of data on each dispensary now stashed away in a JSON file, I could move on to scraping some simpler sources of data. I promise I'll describe those ones more succinctly.&lt;/p&gt;
&lt;h3&gt;Scraping demographic data&lt;/h3&gt;
&lt;p&gt;Getting demographic data for each dispensary area was actually a bit more challenging than I expected. While US census data is publicly available, it's generally not provided at a very granular or intuitive level. After a good long time searching around different aggregator sites, I came across the Washington Hometown Locator website, which offers up data down to each zip code in the state.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="/img/demo-data.png" alt="demo-data" style="width:80%"&gt;
  &lt;figcaption&gt;Zip code level demographic data from Washington Hometown Locator&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;From here, I wrote a simple script that would:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Extract every zip code from my dataset of dispensary license holders.&lt;/li&gt;
&lt;li&gt;Generate a URL using each of these zip codes to point to the appropriate page on WHL&lt;/li&gt;
&lt;li&gt;Download the table data from each of these pages using the Pandas function pd.read_html. It was actually kind of miraculous to me how simple and effective this method was. Thanks to &lt;a href="https://datatostories.com/"&gt;Young Jeong&lt;/a&gt; for the tip!&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Getting walk and bike scores from the Walkscore API&lt;/h3&gt;
&lt;p&gt;It seemed plausible to me that I could leverage &lt;a href="https://www.walkscore.com/"&gt;Walkscore&lt;/a&gt;'s scoring of how prime a location is according to how walkable it is, and fortunately for me it they've got a public facing API that's free to access (up to a 5000 requests a day). This one was actually as simple as applying for an API key (took about an hour to get it) and writing a script to generate request call URLs by combining address, city, state, zip code, and the lat/lon coordinates for each location. The only complication is that I didn't yet actually have any single data source where providing me with all of this information. For that small reason I actually don't execute this step until late in the code after I've already done quite a bit of cleaning and merging of the data.&lt;/p&gt;
&lt;h2&gt;Cleaning, Combining, and Developing the Data&lt;/h2&gt;
&lt;h3&gt;Merging data without a common key&lt;/h3&gt;
&lt;h3&gt;Determining dispensary density&lt;/h3&gt;
&lt;h2&gt;Exploration and Feature Engineering&lt;/h2&gt;
&lt;h3&gt;Heatmap of correlations&lt;/h3&gt;
&lt;figure align="middle"&gt;
  &lt;img src="/img/heatmap.png" alt="heatmap" style="width:100%"&gt;
  &lt;figcaption&gt;Correlation map of a selection of features and the target variable&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3&gt;Univariate regression plots of each feature with target variable&lt;/h3&gt;
&lt;p&gt;One of the primary modes of feature engineering that I focused on in attempting to improve my model was feature transformations. These are cases where I could tell by looking at a plot of a feature vs the target variable that there was in fact a relationship there, but that it wasn't simply linear. &lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="/img/rel_plots.png" alt="rel-plots" style="width:100%"&gt;
  &lt;figcaption&gt;Scatterplots showing selected features relative to both log- and non-log-transformed target variable.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure align="middle"&gt;
  &lt;img src="/img/log_transforms.png" alt="log-transforms" style="width:100%"&gt;
  &lt;figcaption&gt;Scatterplots showing selected log-transformed features relative to both log- and non-log-transformed target variable.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2&gt;Building and Refining a Model&lt;/h2&gt;
&lt;h3&gt;Building an MVP (Minimum Viable Product) with multivariate linear regression&lt;/h3&gt;
&lt;p&gt;Once I had all my data combined and formatted into a &lt;a href="https://en.wikipedia.org/wiki/Design_matrix"&gt;design matrix&lt;/a&gt;, it was time to run my first linear regression and get an idea of my baseline model performance. Mind you, the purpose here is not yet to have a particularly &lt;em&gt;good&lt;/em&gt; model, but simply to have established a simple, functioning pipeline that we can quickly and easily iterate on. &lt;/p&gt;
&lt;p&gt;Of course, I couldn't simply allow myself to just throw &lt;em&gt;all&lt;/em&gt; of my features into a linear regression model and call that goodâ€”at this point I was looking at 60 continuous variables, many of them highly covariant, and had just shy of 400 observations in my data. A ratio like that is a certain recipe for an unstable solutionspace that overfits every time, and I wouldn't wish it even on the model of my worst enemy. So I ran a quick &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_top_corrs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total_sales&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Given dataframe, return a list of the n features most strongly correlated &lt;/span&gt;
&lt;span class="sd"&gt;    with the target variable&lt;/span&gt;
&lt;span class="sd"&gt;    Input: data in a Pandas dataframe, int for how many features desired&lt;/span&gt;
&lt;span class="sd"&gt;    (default 15), string of column name representing target variable&lt;/span&gt;
&lt;span class="sd"&gt;    Output: features, a list of strings representing the column names of just &lt;/span&gt;
&lt;span class="sd"&gt;    the top n features&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;numeric_corrs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_dtypes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corr&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;top_corrs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numeric_corrs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_corrs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_processed_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_top_corrs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total_sales&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and voila! I had myself a dataframe containing only my 15 features most highly correlated with my target variable. Sure, 15 was an arbitrary choice that just felt a bit better than 60 and I knew there were all kinds of imperfections in the dataâ€¦but this was something I could regress on with a bit less guilt.&lt;/p&gt;
&lt;p&gt;From there I generated an 80/20 train/test split (with a fixed random seed for repeatability, of course), fit a vanilla linear regression model on the X, y values from my training set and then used the model to predict the y values based on only the X from my training set. Unfortunately the precise results from my first run have been lost to the iterative ages, but if memory serves my initial training R^2 came out to about 0.3, with my test R^2 a bit closer to 0.2. I stuck the last few steps in a for-loop and iterated through 50 random seeds to generate a plot much like the following to show me how much it was varying just based on how the dataset was randomly split. Note that the plot below was actually generated after I had already started a bit of feature engineering.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="/img/mvp.png" alt="mvp" style="width:60%"&gt;
  &lt;figcaption&gt;Scoring metrics from initial regression run prior to data processing and feature engineering&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;At this point I could only loosely argue that my model was performing better than if had simply predicted the mean dispensary revenue every time. Plenty of room to improve!&lt;/p&gt;
&lt;h3&gt;Optimizing our model and feature selection with feature scaling and regularization through lasso regression&lt;/h3&gt;
&lt;p&gt;This is the point where I entered the iterative phase, which basically meant living in a feedback loop bouncing back and forth between model experimentation and feature engineeringâ€”until it didn't seem like I was going to get performance much better within the time constraints at hand. &lt;/p&gt;
&lt;p&gt;To do this, I wrote a pipeline function to perform the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Import processed data into a Pandas dataframe&lt;/li&gt;
&lt;li&gt;Scale each feature by subtracting off its mean value and dividing it by its standard deviation.&lt;/li&gt;
&lt;li&gt;Split the dataframe ("randomly") into a training set containing 80% of the observations and a test set containing the other 20%.&lt;/li&gt;
&lt;li&gt;Stash the 20% test set far far away where my model couldn't see it.&lt;/li&gt;
&lt;li&gt;Instantiate a regression model through sci-kits learn. I used:&lt;/li&gt;
&lt;li&gt;Linear regression&lt;/li&gt;
&lt;li&gt;Ridge regression (with alpha parameter)&lt;/li&gt;
&lt;li&gt;Lasso regression (with alpha parameter)&lt;/li&gt;
&lt;li&gt;Perform a 5-way cross validation split on the training data (so splitting it further into 5 groups, each containing 80/20 split of the 80% training set.&lt;/li&gt;
&lt;li&gt;Train the model on each cross-validation training set, then compute error metrics using the cv test sets.&lt;/li&gt;
&lt;li&gt;Take the average error over the set of 5 runs and log that in a report to be referenced later when comparing models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that framework in place, I proceeded to subject my data to a battery of experimental models. Because I knew that I have way too many features for such limited data, I decided to try out ridge and lasso regression regularization methods as their cost functions are effective for reducing feature count and suppressing multicollinearity, which I certainly had plenty of. &lt;/p&gt;
&lt;p&gt;Though sklearn has functions in place (RidgeCV and LassoCV) that automatically optimize the penalty hyperparameter ${\lambda}$ (or $\alpha$ within sklearn) through cross validation, I decided to recode the process by hand. Because like they say, reinventing the wheel is the best way to know your way around it. I mean, I bet &lt;em&gt;somebody's&lt;/em&gt; set that before.&lt;/p&gt;
&lt;p&gt;So I set up a for loop to iteratively run models on the data while tweaking the value for $\lambda$ on a logarithmic scale until I'd identified the range of values that tended to minimize the mean squared error of the model when run on the test data. From there I modified my range and zoomed in until I'd identified values that seemed "optimal enough" (90.658 for ridge regression and 0.0172 for lasso). It was time to make a choice. The results of final runs with each model are copied below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Rows&lt;/th&gt;
&lt;th&gt;Cols&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Train R^2&lt;/th&gt;
&lt;th&gt;Test R^2&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Ridge regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.5026&lt;/td&gt;
&lt;td&gt;0.3439&lt;/td&gt;
&lt;td&gt;0.0977&lt;/td&gt;
&lt;td&gt;0.3126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.4676&lt;/td&gt;
&lt;td&gt;0.3782&lt;/td&gt;
&lt;td&gt;0.0931&lt;/td&gt;
&lt;td&gt;0.3051&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Their performance was close, but ultimately I chose the lasso regression as my preferred model for this situation both because it performed better in terms of my error metrics an also because it has the effect of actually zeroing out the coefficients on underperforming features rather than just suppressing them to low values. This feature strikes me as just a touch better generalization and more easily interpretable, too.&lt;/p&gt;
&lt;p&gt;Final model selected, I went ahead and dug up my true test data from the vault and ran a final prediction on the as-yet-unseen test data.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Rows&lt;/th&gt;
&lt;th&gt;Cols&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Train R^2&lt;/th&gt;
&lt;th&gt;Test R^2&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Ridge regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.5026&lt;/td&gt;
&lt;td&gt;0.3439&lt;/td&gt;
&lt;td&gt;0.0977&lt;/td&gt;
&lt;td&gt;0.3126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.4676&lt;/td&gt;
&lt;td&gt;0.3782&lt;/td&gt;
&lt;td&gt;0.0931&lt;/td&gt;
&lt;td&gt;0.3051&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Final lasso regression&lt;/td&gt;
&lt;td&gt;291&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;log_total_sales&lt;/td&gt;
&lt;td&gt;0.4546&lt;/td&gt;
&lt;td&gt;0.3781&lt;/td&gt;
&lt;td&gt;0.1094&lt;/td&gt;
&lt;td&gt;0.3307&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Slightly worse than our cross validated results, that's to be expected. While we've reached a model that is certainly more robust than our original regression, our performance is still lackluster. There's obviously plenty of room for improvement if this ever gets picked up again.&lt;/p&gt;
&lt;figure align="middle"&gt;
  &lt;img src="/img/residuals.png" alt="residuals" style="width:80%"&gt;
  &lt;figcaption&gt;Scatterplot of final model performance, where green is true values of test data observations, red is model predictions, and blue is the residuals (error) of each prediction.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2&gt;So What Insights Can We Glean?&lt;/h2&gt;
&lt;p&gt;misc, number_of_reviews, population_in_group_qrtrs, population_density&lt;/p&gt;
&lt;h3&gt;Descaling and translating our coefficients into terms we can understand&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/th&gt;
&lt;th align="right"&gt;&lt;strong&gt;Scaled Coefficients&lt;/strong&gt;&lt;/th&gt;
&lt;th align="right"&gt;&lt;strong&gt;Unscaled Coefficients&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;log_number_of_reviews&lt;/td&gt;
&lt;td align="right"&gt;0.15479356&lt;/td&gt;
&lt;td align="right"&gt;0.22254295&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;log_population_density&lt;/td&gt;
&lt;td align="right"&gt;0.07910167&lt;/td&gt;
&lt;td align="right"&gt;0.09257050&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;prerolls&lt;/td&gt;
&lt;td align="right"&gt;0.02178097&lt;/td&gt;
&lt;td align="right"&gt;0.00023536&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;owner_occupied_housing_units_#&lt;/td&gt;
&lt;td align="right"&gt;0.01376879&lt;/td&gt;
&lt;td align="right"&gt;0.00000379&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;all_products&lt;/td&gt;
&lt;td align="right"&gt;0.01267409&lt;/td&gt;
&lt;td align="right"&gt;0.00002623&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;log_population_in_group_qrtrs&lt;/td&gt;
&lt;td align="right"&gt;0.01068558&lt;/td&gt;
&lt;td align="right"&gt;0.01412068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;number_of_reviews&lt;/td&gt;
&lt;td align="right"&gt;0.00977085&lt;/td&gt;
&lt;td align="right"&gt;0.00007235&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;per_capita_income&lt;/td&gt;
&lt;td align="right"&gt;0.00622086&lt;/td&gt;
&lt;td align="right"&gt;0.00000056&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;misc&lt;/td&gt;
&lt;td align="right"&gt;0.00205136&lt;/td&gt;
&lt;td align="right"&gt;0.00001326&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Future Work&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Collect more data&lt;/li&gt;
&lt;li&gt;More extensive exploratory data analysis&lt;/li&gt;
&lt;li&gt;Time series projections&lt;/li&gt;
&lt;li&gt;Look at revenue &lt;em&gt;growth&lt;/em&gt; as a target variable&lt;/li&gt;
&lt;li&gt;Experiment with different models&lt;/li&gt;
&lt;li&gt;Suggest optimal locations and product lines&lt;/li&gt;
&lt;/ul&gt;</content><category term="python"></category><category term="data science"></category><category term="web scraping"></category><category term="leafly"></category><category term="marijuana"></category><category term="washington"></category></entry><entry><title>Welcome to Bits 'n' Bots!</title><link href="/articles/2019/04/20/my-first-blog-post/" rel="alternate"></link><published>2019-04-20T12:38:00-07:00</published><updated>2019-04-20T12:57:00-07:00</updated><author><name>Scott Butters</name></author><id>tag:None,2019-04-20:/articles/2019/04/20/my-first-blog-post/</id><summary type="html">&lt;p&gt;Introduction to what the bits 'n' bots blog is about&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Bits 'n' Bots: Alive?&lt;/h1&gt;
&lt;p&gt;Welcome to Bits 'n' Bots, the up and coming repo of my thoughts, musings, projects and passions. Here we'll find a incomplete catalog of the off-the-wall topics I obsess over and explore. This blog will be generally data centric and analytical, a site to host and discuss my data science explorations, but from time to time I expect I'll also venture into topics such as futurist ethics, robotics, and really just about anything related to the impending singularity. Thanks for reading and feel free to reach out if any of my work catches your interest!&lt;/p&gt;</content><category term="python"></category><category term="data science"></category><category term="robots"></category><category term="AI"></category><category term="the singularity"></category></entry></feed>